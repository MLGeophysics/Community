{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6 Welcome to the wiki for Machine Learning in Geophysics. This site is maintained by a group of researchers at the Colorado School of Mines interested in applying machine learning to geophysical investigations. How to explore this website On a Desktop: Currently, there are three main sections to this website shown in the navigation tab at the top of the page. Use these tabs to explore the different aspects of the project! Use the sidebar to the right to explore the contents of the current page and use the sidebar to the left to find all the different pages for this active section/tab. Here is a list of the available sections: Welcome!: An introduction to the MLGeophysics group with details on how to contribute. Minutes: Pages for the minutes from our weekly meetings. Projects: Projects we\u2019ve worked on as a group. Resources: A conglomerate of resources we\u2019d like to share.","title":"Welcome!"},{"location":"#welcome","text":"Welcome to the wiki for Machine Learning in Geophysics. This site is maintained by a group of researchers at the Colorado School of Mines interested in applying machine learning to geophysical investigations. How to explore this website On a Desktop: Currently, there are three main sections to this website shown in the navigation tab at the top of the page. Use these tabs to explore the different aspects of the project! Use the sidebar to the right to explore the contents of the current page and use the sidebar to the left to find all the different pages for this active section/tab. Here is a list of the available sections: Welcome!: An introduction to the MLGeophysics group with details on how to contribute. Minutes: Pages for the minutes from our weekly meetings. Projects: Projects we\u2019ve worked on as a group. Resources: A conglomerate of resources we\u2019d like to share.","title":"Welcome!"},{"location":"contributing/","text":"How to Contribute \u00b6 Here is a guide on how to contribute your content. Working in Markdown Here are some tips for writing documentation in Markdown and here is the guid for mkdocs-material markdown specific to this website format. On GitHub \u00b6 We have set up this website to be automatically generated and deployed via Travis-CI so anyone with write privileges can contribute content and have the website immediately reflect their contributions! To get started, go to the MLGeophysics/Community repository and navigate to the docs directory. From there find the appropriate folder for your contribution and create a new file. Want to create a new section (a new tab on the menu bar)? Then simply make a new directory in the docs directory. Add your content in markdown and preview the page before you finish. Finally commit your changes and wait a few minutes for Travis-CI to deploy the website. On Your Own Machine \u00b6 First, clone the MLGeophysics/Community repository: $ git clone https://github.com/MLGeophysics/Community.git $ cd Community Create a Python virtual environment: $ conda create -n mlgp python = 3 .6 $ conda activate mlgp ( mlgp ) $ pip install -r requirements.txt Now lets serve the website on your machine so that you can add files to the project and immediately see how this looks: ( mlgp ) $ mkdocs serve Open the locally hosted webpage. It should output from the mkdocs serve command and likely would be: http://<YOUR.IP.ADDRESS>:8000 . Make changes and see the results in your web browser. Once you are happy with your contributions, commit and push your changes to the repository!","title":"How to Contribute"},{"location":"contributing/#how-to-contribute","text":"Here is a guide on how to contribute your content. Working in Markdown Here are some tips for writing documentation in Markdown and here is the guid for mkdocs-material markdown specific to this website format.","title":"How to Contribute"},{"location":"contributing/#on-github","text":"We have set up this website to be automatically generated and deployed via Travis-CI so anyone with write privileges can contribute content and have the website immediately reflect their contributions! To get started, go to the MLGeophysics/Community repository and navigate to the docs directory. From there find the appropriate folder for your contribution and create a new file. Want to create a new section (a new tab on the menu bar)? Then simply make a new directory in the docs directory. Add your content in markdown and preview the page before you finish. Finally commit your changes and wait a few minutes for Travis-CI to deploy the website.","title":"On GitHub"},{"location":"contributing/#on-your-own-machine","text":"First, clone the MLGeophysics/Community repository: $ git clone https://github.com/MLGeophysics/Community.git $ cd Community Create a Python virtual environment: $ conda create -n mlgp python = 3 .6 $ conda activate mlgp ( mlgp ) $ pip install -r requirements.txt Now lets serve the website on your machine so that you can add files to the project and immediately see how this looks: ( mlgp ) $ mkdocs serve Open the locally hosted webpage. It should output from the mkdocs serve command and likely would be: http://<YOUR.IP.ADDRESS>:8000 . Make changes and see the results in your web browser. Once you are happy with your contributions, commit and push your changes to the repository!","title":"On Your Own Machine"},{"location":"minutes/18-08-31/","text":"2018 August 31 \u00b6 Create a Conda Virtual Environment \u00b6 Thomas discussed best practices for managing Python packages and recommends nestling all used packages in local Python virtual environments. Thomas highly recommends not using system wide installations. Example conda virtual environment: $ conda create -n mlgp python = 3 .6 $ conda activate mlgp ( mlgp ) $ pip install -r requirements.txt # install whatever you want via pip Get Started with Kerras and TensorFlow \u00b6 Thomas recommends using Kerras with a TensorFlow backend. Keras is easy to use ImageDateGenerator from Keras lets you augment images. Use fit_generator when using ImageDataGenerator U-Net \u00b6 Achieved an accuracy of 78% on validation set. Incorporating seismic attributes may increase accuracy Image scaling is important.","title":"2018 August 31"},{"location":"minutes/18-08-31/#2018-august-31","text":"","title":"2018 August 31"},{"location":"minutes/18-08-31/#create-a-conda-virtual-environment","text":"Thomas discussed best practices for managing Python packages and recommends nestling all used packages in local Python virtual environments. Thomas highly recommends not using system wide installations. Example conda virtual environment: $ conda create -n mlgp python = 3 .6 $ conda activate mlgp ( mlgp ) $ pip install -r requirements.txt # install whatever you want via pip","title":"Create a Conda Virtual Environment"},{"location":"minutes/18-08-31/#get-started-with-kerras-and-tensorflow","text":"Thomas recommends using Kerras with a TensorFlow backend. Keras is easy to use ImageDateGenerator from Keras lets you augment images. Use fit_generator when using ImageDataGenerator","title":"Get Started with Kerras and TensorFlow"},{"location":"minutes/18-08-31/#u-net","text":"Achieved an accuracy of 78% on validation set. Incorporating seismic attributes may increase accuracy Image scaling is important.","title":"U-Net"},{"location":"minutes/18-10-22/","text":"2018 October 22 \u00b6 Location: GRL 107 \u00b6 Group members discussed impressions from SEG annual presentations relating to machine learning. A crude outline of topics discussed follows. Attending: Andy Antoine Thomas Iga Jihyun Bane Arnab Hayden TODO: find room for weekly meetings @ noon (Thomas) prepare next week\u2019s topic slides (Bane + Hayden + Arnab) Windowed CNN for fault classification \u00b6 Problem : \u201cWhere are the faults?\u201d Xin Ming Wu (OG CWP but UT now) Simple synthetics for generating training data for reflectivity model. Artificial faults induced in data via Mines JTK. Results look convincing. Fault classification given by quantizing: fault existence azimuth and dip Windowed approach, assuming fault goes through the center of voxel (??) Faults reconstructed after classification Classifying signals in ambient seismic \u00b6 Problem : \u201cI\u2019m only interested in certain ambient signals.\u201d Fantine Huot (Stanford SEP) Application to DAS and finding specific types of signals Unsupervised, pick, supervised, then rinse+repeat Extrapolating frequencies for improving FWI \u00b6 Problem : \u201cI need lower frequencies for my FWI to work.\u201d Is this approach physically valid? Hopfield network + boltzmann machine used Q: Why and how do NN deal with RTM artifacts \u201cbetter\u201d? Overall fishy, probably waste of time - we already can do RTM FWI updates via NN \u00b6 Problem \u201cMy FWI model updates take a while and aren\u2019t great.\u201d NN weights updated instead of model updates. Q: Is this like what Andy is doing? ML in interpretation \u00b6 Problem \u201cProper salt interpretation takes too much time and expert knowledge.\u201d Salt interpretation is popular. Top, bottom, and existence interpretations. Problem quantifying error in results. Interpreter still required for QC. 2D overused, 3D is more appropriate. Surface wave attenuation \u00b6 Problem \u201cSomeone get these surface waves out of my data please.\u201d So many GANs. Input: noisy data Output: denoised data given by (insert denoising alg. here) Training data: inputs and outputs from (insert denoising alg. here) You could go unsupervised here. Next steps \u00b6 finding sparse represenations of data (dictionary learning) for denoising and interpolation. (Iga, Arnab, Thomas) denoising and monitoring DAS data (Jihyun) Next week \u00b6 Topics Bane - semester project Hayden - \u201cI have research, I got this.\u201d Arnab - existing project Backup - Iga Resources \u00b6 Getting started with VAEs Open source labelled seismic training data","title":"2018 October 22"},{"location":"minutes/18-10-22/#2018-october-22","text":"","title":"2018 October 22"},{"location":"minutes/18-10-22/#location-grl-107","text":"Group members discussed impressions from SEG annual presentations relating to machine learning. A crude outline of topics discussed follows. Attending: Andy Antoine Thomas Iga Jihyun Bane Arnab Hayden TODO: find room for weekly meetings @ noon (Thomas) prepare next week\u2019s topic slides (Bane + Hayden + Arnab)","title":"Location: GRL 107"},{"location":"minutes/18-10-22/#windowed-cnn-for-fault-classification","text":"Problem : \u201cWhere are the faults?\u201d Xin Ming Wu (OG CWP but UT now) Simple synthetics for generating training data for reflectivity model. Artificial faults induced in data via Mines JTK. Results look convincing. Fault classification given by quantizing: fault existence azimuth and dip Windowed approach, assuming fault goes through the center of voxel (??) Faults reconstructed after classification","title":"Windowed CNN for fault classification"},{"location":"minutes/18-10-22/#classifying-signals-in-ambient-seismic","text":"Problem : \u201cI\u2019m only interested in certain ambient signals.\u201d Fantine Huot (Stanford SEP) Application to DAS and finding specific types of signals Unsupervised, pick, supervised, then rinse+repeat","title":"Classifying signals in ambient seismic"},{"location":"minutes/18-10-22/#extrapolating-frequencies-for-improving-fwi","text":"Problem : \u201cI need lower frequencies for my FWI to work.\u201d Is this approach physically valid? Hopfield network + boltzmann machine used Q: Why and how do NN deal with RTM artifacts \u201cbetter\u201d? Overall fishy, probably waste of time - we already can do RTM","title":"Extrapolating frequencies for improving FWI"},{"location":"minutes/18-10-22/#fwi-updates-via-nn","text":"Problem \u201cMy FWI model updates take a while and aren\u2019t great.\u201d NN weights updated instead of model updates. Q: Is this like what Andy is doing?","title":"FWI updates via NN"},{"location":"minutes/18-10-22/#ml-in-interpretation","text":"Problem \u201cProper salt interpretation takes too much time and expert knowledge.\u201d Salt interpretation is popular. Top, bottom, and existence interpretations. Problem quantifying error in results. Interpreter still required for QC. 2D overused, 3D is more appropriate.","title":"ML in interpretation"},{"location":"minutes/18-10-22/#surface-wave-attenuation","text":"Problem \u201cSomeone get these surface waves out of my data please.\u201d So many GANs. Input: noisy data Output: denoised data given by (insert denoising alg. here) Training data: inputs and outputs from (insert denoising alg. here) You could go unsupervised here.","title":"Surface wave attenuation"},{"location":"minutes/18-10-22/#next-steps","text":"finding sparse represenations of data (dictionary learning) for denoising and interpolation. (Iga, Arnab, Thomas) denoising and monitoring DAS data (Jihyun)","title":"Next steps"},{"location":"minutes/18-10-22/#next-week","text":"Topics Bane - semester project Hayden - \u201cI have research, I got this.\u201d Arnab - existing project Backup - Iga","title":"Next week"},{"location":"minutes/18-10-22/#resources","text":"Getting started with VAEs Open source labelled seismic training data","title":"Resources"},{"location":"minutes/18-10-30/","text":"2018 October 30 \u00b6 Location: GRLA 107 \u00b6 Group members discussed Hayden\u2019s project, which involves making decisions when provided a large number of earth models from a stochastic inversion. An outline of conversation content is below. Based on the meeting, we need to provide a tiny bit more structure to the one-slide conversation starters to bring focus towards the ML aspects of projects. Attending: Andy Thomas Iga Jihyun Arnab Hayden Jonah TODO: \u00b6 Decide on a different meeting time using a Doodle poll (Jihyun) Add Naive Bayes to Resources tab (Hayden) Email website version of slide as a pdf to Thomas (Hayden) Provide slide content guidelines to focus on ML aspects in meetups (Thomas) Making decisions using a suite of stochastically inverted earth models (Hayden) \u00b6 Goal : \u201cWhen provided many earth models that all fit data, how do we make a decision?\u201d Input : Rock properties: density, p-velocity, s-velocity, and porosity Framework : Naive Bayes Binary Classifier Output : High vs. low producer classification Details : - Stochastic inversion input: rock physics model + amplitude-vs-offset - Initial labels derived from user-defined production-based cutoff - Many statistical assumptions in stochastic inversion framework Problems : - Consistent misclassifcation near faults. - Dimensionality reduction problems. Questions : - How does Naive Bayes work? - What are the statistical assumptions in the inversion? Are they violated by Naive Bayes? Deltas : - Consider exploring which of the 5,000 models predicts best? - Consider using other classification algorithms RF, SVM, \u2026 - Consider using smaller number of rock properties as inputs to lower dimensionality Next week \u00b6 Topics - Bane - semester project - Arnab - existing project - Backup - Iga","title":"2018 October 30"},{"location":"minutes/18-10-30/#2018-october-30","text":"","title":"2018 October 30"},{"location":"minutes/18-10-30/#location-grla-107","text":"Group members discussed Hayden\u2019s project, which involves making decisions when provided a large number of earth models from a stochastic inversion. An outline of conversation content is below. Based on the meeting, we need to provide a tiny bit more structure to the one-slide conversation starters to bring focus towards the ML aspects of projects. Attending: Andy Thomas Iga Jihyun Arnab Hayden Jonah","title":"Location: GRLA 107"},{"location":"minutes/18-10-30/#todo","text":"Decide on a different meeting time using a Doodle poll (Jihyun) Add Naive Bayes to Resources tab (Hayden) Email website version of slide as a pdf to Thomas (Hayden) Provide slide content guidelines to focus on ML aspects in meetups (Thomas)","title":"TODO:"},{"location":"minutes/18-10-30/#making-decisions-using-a-suite-of-stochastically-inverted-earth-models-hayden","text":"Goal : \u201cWhen provided many earth models that all fit data, how do we make a decision?\u201d Input : Rock properties: density, p-velocity, s-velocity, and porosity Framework : Naive Bayes Binary Classifier Output : High vs. low producer classification Details : - Stochastic inversion input: rock physics model + amplitude-vs-offset - Initial labels derived from user-defined production-based cutoff - Many statistical assumptions in stochastic inversion framework Problems : - Consistent misclassifcation near faults. - Dimensionality reduction problems. Questions : - How does Naive Bayes work? - What are the statistical assumptions in the inversion? Are they violated by Naive Bayes? Deltas : - Consider exploring which of the 5,000 models predicts best? - Consider using other classification algorithms RF, SVM, \u2026 - Consider using smaller number of rock properties as inputs to lower dimensionality","title":"Making decisions using a suite of stochastically inverted earth models (Hayden)"},{"location":"minutes/18-10-30/#next-week","text":"Topics - Bane - semester project - Arnab - existing project - Backup - Iga","title":"Next week"},{"location":"minutes/18-11-06/","text":"2018 November 06 \u00b6 Location: GRLA 107 \u00b6 Attendees clarified a hodge-podge of confusing topics and jargon outlined below. These topics and jargon are regularly encounted in ML literature - they are rarely explained in most papers and can inhibit getting started in ML. Next week, Bane\u2019s project is the primary topic of focus. A VAE summary and PyTorch vs. Keras discussion are backup topics. For the remainder of Fall 2018 we are meeting in Coors Tech 282. Attending: Andy Antoine Thomas Iga Jihyun Jonah Xiaodan Hayden TODO: \u00b6 Post NN video (Thomas) Compile slide for next week (Bane) Begin compiling VAE summary (Iga, Andy, Thomas) Begin compiling PyTorch vs. Keras materials (Jihyun) Problems \u00b6 Input data spans several orders of magnitude, large amplitudes dominate. Consider weighting the residuals (like a weighted norm!). GANs are hard to train? Whomp whomp. Jargon: Variational method specific \u00b6 variational distribution : A probability distribution derived using variational methods. Such as Monte Carlo Markov Chain (MCMC). amortized inference : Removing memoryless/independence property of traditional statistics. mean field inference : ??? ELBO : A method related to variational methods. Evidence Lower BOund. Jargon: General \u00b6 Convolutional vs. Deconvolutional layer meaning : Convolutional layers apply a filter via convolution. Deconvolutional layers are really just the transpose of a convolutional layer. The term deconvolution isn\u2019t used correctly in many ML papers. Back propagation: Application of chain rule to derive gradient for weight updates in a network. What is the difference between Stochastic Gradient Descent and Steepest Gradient Descent? Same thing(ish). Stochastic Gradient Descent is a like a submethod of Steepest Gradient Descent. Stochastic involves using a subset of data, each subset called a batch, to update gradient. Every batch has its own gradient. Updating the gradient this way can help you deal with large amounts of data in a manageable way - and may help get away from local minima. Learning rate: Step-length along direction of gradient you travel while minimizing the loss-function. Epochs: An epoch is complete after all data has been considered in set of gradient updates via Stochastic Gradient Descent. Epochs conclude after all batches have had an opportunity to influence the weights in a network. Batch normalization layers: Normalize the subset of data that is in a specific batch. This actually adds weights that need to be learned during training. It relies on a batch that is large enough to be normally distributed. Could regulate behavior of training. Dropout layers: Upon output, a dropout layer ignore a random subset of weights/activated neurons. Weird form of regularization. Difficult to tune. Antoine discourages using this. Topics of discussion for next week \u00b6 Watch neural network video, post and compile questions. Bane - semester project Touch on VAEs Upcoming/backup topics of discussion \u00b6 Variational Auto-encoders summary and application (Iga, Andy, and Thomas) Arnab - existing project PyTorch vs. Keras","title":"2018 November 06"},{"location":"minutes/18-11-06/#2018-november-06","text":"","title":"2018 November 06"},{"location":"minutes/18-11-06/#location-grla-107","text":"Attendees clarified a hodge-podge of confusing topics and jargon outlined below. These topics and jargon are regularly encounted in ML literature - they are rarely explained in most papers and can inhibit getting started in ML. Next week, Bane\u2019s project is the primary topic of focus. A VAE summary and PyTorch vs. Keras discussion are backup topics. For the remainder of Fall 2018 we are meeting in Coors Tech 282. Attending: Andy Antoine Thomas Iga Jihyun Jonah Xiaodan Hayden","title":"Location: GRLA 107"},{"location":"minutes/18-11-06/#todo","text":"Post NN video (Thomas) Compile slide for next week (Bane) Begin compiling VAE summary (Iga, Andy, Thomas) Begin compiling PyTorch vs. Keras materials (Jihyun)","title":"TODO:"},{"location":"minutes/18-11-06/#problems","text":"Input data spans several orders of magnitude, large amplitudes dominate. Consider weighting the residuals (like a weighted norm!). GANs are hard to train? Whomp whomp.","title":"Problems"},{"location":"minutes/18-11-06/#jargon-variational-method-specific","text":"variational distribution : A probability distribution derived using variational methods. Such as Monte Carlo Markov Chain (MCMC). amortized inference : Removing memoryless/independence property of traditional statistics. mean field inference : ??? ELBO : A method related to variational methods. Evidence Lower BOund.","title":"Jargon: Variational method specific"},{"location":"minutes/18-11-06/#jargon-general","text":"Convolutional vs. Deconvolutional layer meaning : Convolutional layers apply a filter via convolution. Deconvolutional layers are really just the transpose of a convolutional layer. The term deconvolution isn\u2019t used correctly in many ML papers. Back propagation: Application of chain rule to derive gradient for weight updates in a network. What is the difference between Stochastic Gradient Descent and Steepest Gradient Descent? Same thing(ish). Stochastic Gradient Descent is a like a submethod of Steepest Gradient Descent. Stochastic involves using a subset of data, each subset called a batch, to update gradient. Every batch has its own gradient. Updating the gradient this way can help you deal with large amounts of data in a manageable way - and may help get away from local minima. Learning rate: Step-length along direction of gradient you travel while minimizing the loss-function. Epochs: An epoch is complete after all data has been considered in set of gradient updates via Stochastic Gradient Descent. Epochs conclude after all batches have had an opportunity to influence the weights in a network. Batch normalization layers: Normalize the subset of data that is in a specific batch. This actually adds weights that need to be learned during training. It relies on a batch that is large enough to be normally distributed. Could regulate behavior of training. Dropout layers: Upon output, a dropout layer ignore a random subset of weights/activated neurons. Weird form of regularization. Difficult to tune. Antoine discourages using this.","title":"Jargon: General"},{"location":"minutes/18-11-06/#topics-of-discussion-for-next-week","text":"Watch neural network video, post and compile questions. Bane - semester project Touch on VAEs","title":"Topics of discussion for next week"},{"location":"minutes/18-11-06/#upcomingbackup-topics-of-discussion","text":"Variational Auto-encoders summary and application (Iga, Andy, and Thomas) Arnab - existing project PyTorch vs. Keras","title":"Upcoming/backup topics of discussion"},{"location":"minutes/18-11-12/","text":"2018 November 12 \u00b6 Location: Coors Tech \u00b6 Attending: Andy Antoine Thomas Iga Jihyun Jonah Bane TODO: \u00b6 Begin compiling VAE summary (Iga, Andy, Thomas) Begin compiling PyTorch vs. Keras materials (Jihyun) Problem \u00b6 Bane has a ton of hydro flow models and corresponding data. What geophysical data is most appropriate for detecting flow? Does not have access to flow simulator. Questions \u00b6 This looks like a sensitivity analysis? What variogram was used to generate your models? Comments \u00b6 Speed up flow simulation for this particular. Explore how NN performs compared to flow inversion (see Joe C. PhD thesis) Train NN with current model+flow. Recurrent NN seems appropriate. You could generate more models for testing if you had the variogram. \u201cSimplify to a point that is easy to generalize from.\u201d -AG Website changes \u00b6 Projects tab (coming soon) Jupyter notebook on website? (coming soon) Topics of discussion for next week \u00b6 Touch on VAEs Upcoming/backup topics of discussion \u00b6 Variational Auto-encoders summary and application (Iga, Andy, and Thomas) Arnab - existing project PyTorch vs. Keras","title":"2018 November 12"},{"location":"minutes/18-11-12/#2018-november-12","text":"","title":"2018 November 12"},{"location":"minutes/18-11-12/#location-coors-tech","text":"Attending: Andy Antoine Thomas Iga Jihyun Jonah Bane","title":"Location: Coors Tech"},{"location":"minutes/18-11-12/#todo","text":"Begin compiling VAE summary (Iga, Andy, Thomas) Begin compiling PyTorch vs. Keras materials (Jihyun)","title":"TODO:"},{"location":"minutes/18-11-12/#problem","text":"Bane has a ton of hydro flow models and corresponding data. What geophysical data is most appropriate for detecting flow? Does not have access to flow simulator.","title":"Problem"},{"location":"minutes/18-11-12/#questions","text":"This looks like a sensitivity analysis? What variogram was used to generate your models?","title":"Questions"},{"location":"minutes/18-11-12/#comments","text":"Speed up flow simulation for this particular. Explore how NN performs compared to flow inversion (see Joe C. PhD thesis) Train NN with current model+flow. Recurrent NN seems appropriate. You could generate more models for testing if you had the variogram. \u201cSimplify to a point that is easy to generalize from.\u201d -AG","title":"Comments"},{"location":"minutes/18-11-12/#website-changes","text":"Projects tab (coming soon) Jupyter notebook on website? (coming soon)","title":"Website changes"},{"location":"minutes/18-11-12/#topics-of-discussion-for-next-week","text":"Touch on VAEs","title":"Topics of discussion for next week"},{"location":"minutes/18-11-12/#upcomingbackup-topics-of-discussion","text":"Variational Auto-encoders summary and application (Iga, Andy, and Thomas) Arnab - existing project PyTorch vs. Keras","title":"Upcoming/backup topics of discussion"},{"location":"minutes/18-11-26/","text":"2018 November 26 \u00b6 Location: Coors Tech \u00b6 Attending: Andy Antoine Thomas Iga Jihyun Bane Hayden TODO \u00b6 Post theorem relating sparsity, DoF, and latent space (compressed dimension) (Thomas) Book Coors Tech 381 for Spring semester (Andy) Send Andy KL-divergence ICA paper (Thomas) Key takeaways \u00b6 We now have basic understanding on autoencoders and variational autoencoders. Problems \u00b6 Bane has a high dimensional dataset - reduces dimensionality using PCA. How to pick number of dimensions to keep after PCA? Consider looking at the explained variance of each principle component, only keeping components that explain most of the variance will lead to lower reconstruction error. Need multiple class classification capabilities. Consider a multivariate regression approach. Questions \u00b6 Why use Leaky ReLu? Because it allows values below zero to be outputted. Also, there is no gradient for ReLu inputs below zero (maybe not a problem). Why call it z_log_sigma ? Because it is in VAE tutorials and the loss function code is simpler after taking a logarithm. Where does the kl_loss definition in vae_loss() come from? Look into independent component analysis (ICA) papers and try rederiving. Why is total_loss variable in vae_loss() computing the mean? Typo? Comments \u00b6 Leaky ReLu is expensive - maybe - but could help with vanishing gradient problem - maybe. Note the use of K. in the definition of loss function in VAE. Note that decresing the loss function regularization pushes standard deviations to zero, making the VAE more deterministic. Topics of discussion for next week \u00b6 (???) Upcoming/backup topics of discussion \u00b6 Arnab - existing project PyTorch vs. Keras","title":"2018 November 26"},{"location":"minutes/18-11-26/#2018-november-26","text":"","title":"2018 November 26"},{"location":"minutes/18-11-26/#location-coors-tech","text":"Attending: Andy Antoine Thomas Iga Jihyun Bane Hayden","title":"Location: Coors Tech"},{"location":"minutes/18-11-26/#todo","text":"Post theorem relating sparsity, DoF, and latent space (compressed dimension) (Thomas) Book Coors Tech 381 for Spring semester (Andy) Send Andy KL-divergence ICA paper (Thomas)","title":"TODO"},{"location":"minutes/18-11-26/#key-takeaways","text":"We now have basic understanding on autoencoders and variational autoencoders.","title":"Key takeaways"},{"location":"minutes/18-11-26/#problems","text":"Bane has a high dimensional dataset - reduces dimensionality using PCA. How to pick number of dimensions to keep after PCA? Consider looking at the explained variance of each principle component, only keeping components that explain most of the variance will lead to lower reconstruction error. Need multiple class classification capabilities. Consider a multivariate regression approach.","title":"Problems"},{"location":"minutes/18-11-26/#questions","text":"Why use Leaky ReLu? Because it allows values below zero to be outputted. Also, there is no gradient for ReLu inputs below zero (maybe not a problem). Why call it z_log_sigma ? Because it is in VAE tutorials and the loss function code is simpler after taking a logarithm. Where does the kl_loss definition in vae_loss() come from? Look into independent component analysis (ICA) papers and try rederiving. Why is total_loss variable in vae_loss() computing the mean? Typo?","title":"Questions"},{"location":"minutes/18-11-26/#comments","text":"Leaky ReLu is expensive - maybe - but could help with vanishing gradient problem - maybe. Note the use of K. in the definition of loss function in VAE. Note that decresing the loss function regularization pushes standard deviations to zero, making the VAE more deterministic.","title":"Comments"},{"location":"minutes/18-11-26/#topics-of-discussion-for-next-week","text":"(???)","title":"Topics of discussion for next week"},{"location":"minutes/18-11-26/#upcomingbackup-topics-of-discussion","text":"Arnab - existing project PyTorch vs. Keras","title":"Upcoming/backup topics of discussion"},{"location":"minutes/19-01-14/","text":"2019 January 14 \u00b6 Location: Coors Tech \u00b6 Attending: Andy Antoine Thomas Iga Jihyun Bane Key takeaways \u00b6 Alexi has Kafadar DAS dataset Jihyun and Jeff S. have dam seismic data Jihyun, Thomas, Iga are down to work on LANL Kaggle competition >=1hr/wk Problems \u00b6 Jihyun - dispersion curve analysis for anomaly detection in dams Iga - separate engine noise from seismic data Andy - extend GAN+inversion application to 1D well logs and 1D inversion (MT?) Bane - still formulating the problem with huge amount of flow data. Questions \u00b6 Comments \u00b6 Maybe structural analysis literature has ML-based anomaly detection for Jihyun. Bane may try finding (1) which wells will leak, or (2) depth to leak, or (3) time since leak. Topics of discussion for next week \u00b6 LANL Kaggle competition data overview and preliminary results Upcoming/backup topics of discussion \u00b6 Kaggle competition paper discussion","title":"2019 January 14"},{"location":"minutes/19-01-14/#2019-january-14","text":"","title":"2019 January 14"},{"location":"minutes/19-01-14/#location-coors-tech","text":"Attending: Andy Antoine Thomas Iga Jihyun Bane","title":"Location: Coors Tech"},{"location":"minutes/19-01-14/#key-takeaways","text":"Alexi has Kafadar DAS dataset Jihyun and Jeff S. have dam seismic data Jihyun, Thomas, Iga are down to work on LANL Kaggle competition >=1hr/wk","title":"Key takeaways"},{"location":"minutes/19-01-14/#problems","text":"Jihyun - dispersion curve analysis for anomaly detection in dams Iga - separate engine noise from seismic data Andy - extend GAN+inversion application to 1D well logs and 1D inversion (MT?) Bane - still formulating the problem with huge amount of flow data.","title":"Problems"},{"location":"minutes/19-01-14/#questions","text":"","title":"Questions"},{"location":"minutes/19-01-14/#comments","text":"Maybe structural analysis literature has ML-based anomaly detection for Jihyun. Bane may try finding (1) which wells will leak, or (2) depth to leak, or (3) time since leak.","title":"Comments"},{"location":"minutes/19-01-14/#topics-of-discussion-for-next-week","text":"LANL Kaggle competition data overview and preliminary results","title":"Topics of discussion for next week"},{"location":"minutes/19-01-14/#upcomingbackup-topics-of-discussion","text":"Kaggle competition paper discussion","title":"Upcoming/backup topics of discussion"},{"location":"minutes/19-01-28/","text":"2019 January 28 \u00b6 Location: Coors Tech \u00b6 Attending : \u00b6 Thomas Iga Jihyun Bane Rosie Key takeaways \u00b6 Jihyun got an MSE of 1.5-ish in the Kaggle competition (top 200) Rosie is working on a cool 2D CNN senior design project for fault detection Jihyun will give a brief tutorial on boosting next week Problems \u00b6 Rosie - getting started with fault detection Thomas - how can we scale LANL project features without precomputing them? Comments \u00b6 Rosie is working with 3D migrated DAS images Her inputs are two different migrated images from active and passive sources Her output is a fault map/volume She could try a 2D CNN in Keras to start LANL Kaggle kernels precompute features before deriving scaling Topics of discussion for next week \u00b6 Boosting tutorial from Jihyun","title":"2019 January 28"},{"location":"minutes/19-01-28/#2019-january-28","text":"","title":"2019 January 28"},{"location":"minutes/19-01-28/#location-coors-tech","text":"","title":"Location: Coors Tech"},{"location":"minutes/19-01-28/#attending","text":"Thomas Iga Jihyun Bane Rosie","title":"Attending:"},{"location":"minutes/19-01-28/#key-takeaways","text":"Jihyun got an MSE of 1.5-ish in the Kaggle competition (top 200) Rosie is working on a cool 2D CNN senior design project for fault detection Jihyun will give a brief tutorial on boosting next week","title":"Key takeaways"},{"location":"minutes/19-01-28/#problems","text":"Rosie - getting started with fault detection Thomas - how can we scale LANL project features without precomputing them?","title":"Problems"},{"location":"minutes/19-01-28/#comments","text":"Rosie is working with 3D migrated DAS images Her inputs are two different migrated images from active and passive sources Her output is a fault map/volume She could try a 2D CNN in Keras to start LANL Kaggle kernels precompute features before deriving scaling","title":"Comments"},{"location":"minutes/19-01-28/#topics-of-discussion-for-next-week","text":"Boosting tutorial from Jihyun","title":"Topics of discussion for next week"},{"location":"minutes/19-02-25/","text":"2019 February 25 \u00b6 Location: Coors Tech \u00b6 Attending : \u00b6 Thomas Iga Jihyun Bane Hayden Andy Antoine Key takeaways \u00b6 Hayden has lots of feedback in the comments. Problems \u00b6 Hayden has a ridiculously high number of feature dimensions How to validate with low number of wells? If trained successfully, would a model from here work in the field? Comments \u00b6 Polar coordinate interpolation to avoid nans PCA to reduce dimensionality Multi-dimensional scaling for data dimensionality reduction Check error on slide 4 (normalize between -1 and 1) A super low training loss may indicate over training. Consider cross-validating across wells and models per well. A dropout layer may help you reduce over training. Consider leveraging spatial correlation with convolutional layers. Space filling curves may help reorder cells into an ordered 1D sequence. Consider autoencoding data then training network (alternative to PCA). Consider intelligently selecting validation set (not randomly). Topics of discussion for next week \u00b6 Bin will discuss his project and get feedback","title":"2019 February 25"},{"location":"minutes/19-02-25/#2019-february-25","text":"","title":"2019 February 25"},{"location":"minutes/19-02-25/#location-coors-tech","text":"","title":"Location: Coors Tech"},{"location":"minutes/19-02-25/#attending","text":"Thomas Iga Jihyun Bane Hayden Andy Antoine","title":"Attending:"},{"location":"minutes/19-02-25/#key-takeaways","text":"Hayden has lots of feedback in the comments.","title":"Key takeaways"},{"location":"minutes/19-02-25/#problems","text":"Hayden has a ridiculously high number of feature dimensions How to validate with low number of wells? If trained successfully, would a model from here work in the field?","title":"Problems"},{"location":"minutes/19-02-25/#comments","text":"Polar coordinate interpolation to avoid nans PCA to reduce dimensionality Multi-dimensional scaling for data dimensionality reduction Check error on slide 4 (normalize between -1 and 1) A super low training loss may indicate over training. Consider cross-validating across wells and models per well. A dropout layer may help you reduce over training. Consider leveraging spatial correlation with convolutional layers. Space filling curves may help reorder cells into an ordered 1D sequence. Consider autoencoding data then training network (alternative to PCA). Consider intelligently selecting validation set (not randomly).","title":"Comments"},{"location":"minutes/19-02-25/#topics-of-discussion-for-next-week","text":"Bin will discuss his project and get feedback","title":"Topics of discussion for next week"},{"location":"minutes/19-03-11/","text":"2019 March 11 \u00b6 Location: Coors Tech \u00b6 Attending : \u00b6 Thomas Iga Jihyun Hayden Andy Antoine Bin Key takeaways \u00b6 Variability in NN performance observed when using tanh vs. sigmoid Hard to verify real-world applicability with Bins project Bin needs real data before understanding results of synthetics Consider using hot-encoding to turn your regression problem into a classification problem Problems \u00b6 Hayden is not sure whether to use tanh or sigmoid tanh shows more variablity compared to sigmoid Bin may have a dynamic range problem on inputs 0, 1, 2, \u2026 254, 255 1E-12 to 1 Bin\u2019s validation and training data are extremely similar so cross-validation is probably biased. Without prior setup, Google Cloud will use CPU by default. Comments \u00b6 tanh outputs [-1,1] whereas sigmoid outputs [0,1], perhaps explaining accuracy disparity Hayden is observing Bin did a PhD involving earthquake mechanics and finite element modelling. Is currently trying to take slip-rate outputs to time to failure. Inputs to FEM: material properties and physical parameters Outputs of FEM: slip rate in a volume Inputs to NN: slip rate on a sliced portion of volume Outputs of NN: time to failure Using transfer learning approach TLA option 1: reuse architecture TLA option 2: reuse architecture + learned weights TLA option 2: forward, get features, use features to train separate simpler NN Bin\u2019s process notes: Wants time to failure accuracy of one year InceptionV3 Google Cloud CPU forward 600 images in 1.5 minutes VGG16 Google Cloud CPU forward 600 images in 5 minutes Avoids high forward cost by using network to get features that are used to train a separate simple NN. Duplicate along each channel for input to both image-based pre-trained models as if representing RGB data Summarizes regression output by avg. pooling at the last layer of each model Google Cloud is currently being used by Bin right now make sure GC knows you want GPUs InceptionV3 takes longer to train, but it\u2019s output is bigger VGG16 is faster to train, but its output is smaller Freezing layers, and adding an initial layer (to help with dynamic range), is a bad idea. All future weights are influenced by past weights. Consider using hot-encoding to turn your regression problem into a classification problem Year 1 = [1, 0, 0, \u2026 0] Year 2 = [0, 1, 0, \u2026 0] Topics of discussion for next week \u00b6 TBD","title":"2019 March 11"},{"location":"minutes/19-03-11/#2019-march-11","text":"","title":"2019 March 11"},{"location":"minutes/19-03-11/#location-coors-tech","text":"","title":"Location: Coors Tech"},{"location":"minutes/19-03-11/#attending","text":"Thomas Iga Jihyun Hayden Andy Antoine Bin","title":"Attending:"},{"location":"minutes/19-03-11/#key-takeaways","text":"Variability in NN performance observed when using tanh vs. sigmoid Hard to verify real-world applicability with Bins project Bin needs real data before understanding results of synthetics Consider using hot-encoding to turn your regression problem into a classification problem","title":"Key takeaways"},{"location":"minutes/19-03-11/#problems","text":"Hayden is not sure whether to use tanh or sigmoid tanh shows more variablity compared to sigmoid Bin may have a dynamic range problem on inputs 0, 1, 2, \u2026 254, 255 1E-12 to 1 Bin\u2019s validation and training data are extremely similar so cross-validation is probably biased. Without prior setup, Google Cloud will use CPU by default.","title":"Problems"},{"location":"minutes/19-03-11/#comments","text":"tanh outputs [-1,1] whereas sigmoid outputs [0,1], perhaps explaining accuracy disparity Hayden is observing Bin did a PhD involving earthquake mechanics and finite element modelling. Is currently trying to take slip-rate outputs to time to failure. Inputs to FEM: material properties and physical parameters Outputs of FEM: slip rate in a volume Inputs to NN: slip rate on a sliced portion of volume Outputs of NN: time to failure Using transfer learning approach TLA option 1: reuse architecture TLA option 2: reuse architecture + learned weights TLA option 2: forward, get features, use features to train separate simpler NN Bin\u2019s process notes: Wants time to failure accuracy of one year InceptionV3 Google Cloud CPU forward 600 images in 1.5 minutes VGG16 Google Cloud CPU forward 600 images in 5 minutes Avoids high forward cost by using network to get features that are used to train a separate simple NN. Duplicate along each channel for input to both image-based pre-trained models as if representing RGB data Summarizes regression output by avg. pooling at the last layer of each model Google Cloud is currently being used by Bin right now make sure GC knows you want GPUs InceptionV3 takes longer to train, but it\u2019s output is bigger VGG16 is faster to train, but its output is smaller Freezing layers, and adding an initial layer (to help with dynamic range), is a bad idea. All future weights are influenced by past weights. Consider using hot-encoding to turn your regression problem into a classification problem Year 1 = [1, 0, 0, \u2026 0] Year 2 = [0, 1, 0, \u2026 0]","title":"Comments"},{"location":"minutes/19-03-11/#topics-of-discussion-for-next-week","text":"TBD","title":"Topics of discussion for next week"},{"location":"minutes/19-04-19/","text":"2019 April 8 \u00b6 Location: Coors Tech \u00b6 Attending : \u00b6 Iga Jihyun Andy Antoine Bin Julian Bane Thomas Key takeaways \u00b6 Tensorflow officially suppports Keras Tensorflow has new probablistic and open data distribuition tools EDAA is a more formal group for us to potentially present/participate in We now have nice, clean, open, RILD well log data from Kansas Andy has impressive GAN puns. Problems \u00b6 Is 4000 wells enough to properly train a GAN? Is Tensorflow\u2019s official embedding of Keras better than Keras? What are the unites of the well data? Comments \u00b6 Slides for TF updates are on Slack Slides for EDAA are on Slack Well log data for GAN (open data cleaned by Andy) 4000 wells 400 - 4000 ft 1 ft spacing in depth x-y coordinates of wells not in file, but available Data is RILD (MT-like) RILD ranges over several orders of magnitude We should use log(RILD) as inputs All well logs are from Kansas \u201cLet\u2019s build the best GAN we can.\u201d - Andy \u201cAll we need is a good G.\u201d - Andy Downsampled by linear averaging every 60 ft. 3600 samples per well (full resolution) 60 samples per well (downsampled) 100 MB file (full resolution link pending) 2 MB file (downsampled on Slack) Meeting didn\u2019t go as planned timeline, too much in the meeting Topics of discussion for next week \u00b6 Breaking into three groups for pair programming of GAN Two groups for network structure: Andy, Bin, Julian, Iga, Jihyun One group for data viz: Thomas + Bane","title":"2019 April 8"},{"location":"minutes/19-04-19/#2019-april-8","text":"","title":"2019 April 8"},{"location":"minutes/19-04-19/#location-coors-tech","text":"","title":"Location: Coors Tech"},{"location":"minutes/19-04-19/#attending","text":"Iga Jihyun Andy Antoine Bin Julian Bane Thomas","title":"Attending:"},{"location":"minutes/19-04-19/#key-takeaways","text":"Tensorflow officially suppports Keras Tensorflow has new probablistic and open data distribuition tools EDAA is a more formal group for us to potentially present/participate in We now have nice, clean, open, RILD well log data from Kansas Andy has impressive GAN puns.","title":"Key takeaways"},{"location":"minutes/19-04-19/#problems","text":"Is 4000 wells enough to properly train a GAN? Is Tensorflow\u2019s official embedding of Keras better than Keras? What are the unites of the well data?","title":"Problems"},{"location":"minutes/19-04-19/#comments","text":"Slides for TF updates are on Slack Slides for EDAA are on Slack Well log data for GAN (open data cleaned by Andy) 4000 wells 400 - 4000 ft 1 ft spacing in depth x-y coordinates of wells not in file, but available Data is RILD (MT-like) RILD ranges over several orders of magnitude We should use log(RILD) as inputs All well logs are from Kansas \u201cLet\u2019s build the best GAN we can.\u201d - Andy \u201cAll we need is a good G.\u201d - Andy Downsampled by linear averaging every 60 ft. 3600 samples per well (full resolution) 60 samples per well (downsampled) 100 MB file (full resolution link pending) 2 MB file (downsampled on Slack) Meeting didn\u2019t go as planned timeline, too much in the meeting","title":"Comments"},{"location":"minutes/19-04-19/#topics-of-discussion-for-next-week","text":"Breaking into three groups for pair programming of GAN Two groups for network structure: Andy, Bin, Julian, Iga, Jihyun One group for data viz: Thomas + Bane","title":"Topics of discussion for next week"},{"location":"minutes/19-04-29/","text":"2019 April 29 \u00b6 Location: Coors Tech \u00b6 Attending : \u00b6 Jihyun Andy Thomas Key takeaways \u00b6 Preparing a paper on training a generator on well log data Possible participants: Andy, Thomas, Jihyun, Bin, Bane Well-defined scope, open access publication Problems \u00b6 Data needs to be better cleaned Identify other potential well data types besides resistivity Check the units of the well data Comments \u00b6 Scope of paper Stick to CNN-based GAN Multi-dimensional training data Motivation lack of geosci training data well logs are simple and high fidelity simple geometry ubiquitous oil,water,mining generator no petro-phys model/simulator act as a prior for inversion mention gradients? links different types of logs well log data recovery unsupervised Outline of paper Intro Geology of Kansas GAN Lit review Background on Kansas geology Methods experiment/ data QC/data cleaning GAN Cross-validation spatial stats wavelet transform Fourier transform PCA Results what the clean data looks like short no interp speed training generating statistics cross-validation based on location visualizing well log plots Discussion each result item compare speed vs other methods What did/didn\u2019t the network learn speculation recommendations predictions/future work Contributions of paper published curated dataset published trained network Discussion Task breakdown Thomas: statistical analysis Jihyun: Gan literature review Andy: Data cleaning may need help Download zipped las files from KGS (link on mlgp website) check units: depth and Ohm-m Do our best to handle well geometry ID Nans Handle NaNs ID most popular aliases write functions in .py file Function 1 takes: a well log, mneumonic returns mask for nan, depth Function 2 takes: data, mask, depths returns: \u2018clean\u2019 data Interpolate over no more than 10 ft Split paper off from ML group? We\u2019ll see what interests people have Hani\u2019s like a Pokemon, We gotta catch em. Agenda for next week \u00b6 Clean data Set up communication channels for collaborating over summer Go get a burger","title":"2019 April 29"},{"location":"minutes/19-04-29/#2019-april-29","text":"","title":"2019 April 29"},{"location":"minutes/19-04-29/#location-coors-tech","text":"","title":"Location: Coors Tech"},{"location":"minutes/19-04-29/#attending","text":"Jihyun Andy Thomas","title":"Attending:"},{"location":"minutes/19-04-29/#key-takeaways","text":"Preparing a paper on training a generator on well log data Possible participants: Andy, Thomas, Jihyun, Bin, Bane Well-defined scope, open access publication","title":"Key takeaways"},{"location":"minutes/19-04-29/#problems","text":"Data needs to be better cleaned Identify other potential well data types besides resistivity Check the units of the well data","title":"Problems"},{"location":"minutes/19-04-29/#comments","text":"Scope of paper Stick to CNN-based GAN Multi-dimensional training data Motivation lack of geosci training data well logs are simple and high fidelity simple geometry ubiquitous oil,water,mining generator no petro-phys model/simulator act as a prior for inversion mention gradients? links different types of logs well log data recovery unsupervised Outline of paper Intro Geology of Kansas GAN Lit review Background on Kansas geology Methods experiment/ data QC/data cleaning GAN Cross-validation spatial stats wavelet transform Fourier transform PCA Results what the clean data looks like short no interp speed training generating statistics cross-validation based on location visualizing well log plots Discussion each result item compare speed vs other methods What did/didn\u2019t the network learn speculation recommendations predictions/future work Contributions of paper published curated dataset published trained network Discussion Task breakdown Thomas: statistical analysis Jihyun: Gan literature review Andy: Data cleaning may need help Download zipped las files from KGS (link on mlgp website) check units: depth and Ohm-m Do our best to handle well geometry ID Nans Handle NaNs ID most popular aliases write functions in .py file Function 1 takes: a well log, mneumonic returns mask for nan, depth Function 2 takes: data, mask, depths returns: \u2018clean\u2019 data Interpolate over no more than 10 ft Split paper off from ML group? We\u2019ll see what interests people have Hani\u2019s like a Pokemon, We gotta catch em.","title":"Comments"},{"location":"minutes/19-04-29/#agenda-for-next-week","text":"Clean data Set up communication channels for collaborating over summer Go get a burger","title":"Agenda for next week"},{"location":"minutes/19-05-07/","text":"2019 May 07 \u00b6 Location: Coors Tech \u00b6 Attending \u00b6 Jihyun Andy Thomas Antoine Bin Iga Key takeaways \u00b6 The well log GAN project to be a tutorial-like paper in Geophysics Well log GAN peeps will meet weekly over the summer (TBD) In-person meetings to resume on campus in Fall semester Problems \u00b6 Data still needs to be better cleaned Identify other potential well data types besides resistivity Check the units of the well data Hani\u2019s like a Pokemon, We gotta catch em. Updated notes on well log paper \u00b6 Scope of paper Stick to CNN-based GAN Multi-dimensional training data after 1D success Tone is tutorial-like Motivation lack of geosci training data well logs are simple and high fidelity ubiquitous oil,water,mining generator no petro-phys model/simulator act as a prior for inversion mention gradients? links different types of logs well log data recovery unsupervised Contributions of paper published curated dataset published trained network GAN tutorial foothold into further GAN work Outline of paper Intro Geology of Kansas - postpone to Fall GAN background - Andy, Jihyun, Bin, and Iga RILD background - Andy Methods QC/data cleaning - Andy GAN - Andy, Jihyun, Bin, and Iga Cross-validation - Thomas spatial stats wavelet transform Fourier transform PCA Results QC/data cleaning - Andy GAN - Andy, Jihyun, Bin, and Iga Cross-validation - Thomas spatial stats wavelet transform Fourier transform PCA Discussion each result item Answer: What did/didn\u2019t the network learn speculation on use cases recommendations for changes predictions/future work Task leader breakdown Jihyun: Gan literature review Thomas: statistical analysis Andy: Data cleaning may need help Download zipped las files from KGS (link on mlgp website) check units: depth and Ohm-m Do our best to handle well geometry ID Nans Handle NaNs ID most popular aliases write functions in .py file Function 1 takes: a well log, mneumonic returns mask for nan, depth Function 2 takes: data, mask, depths returns: \u2018clean\u2019 data Interpolate over no more than 10 ft Agenda for next meeting \u00b6 Go get a burger Communicate work done over the summer Line up potential speakers from GP department for the fall","title":"2019 May 07"},{"location":"minutes/19-05-07/#2019-may-07","text":"","title":"2019 May 07"},{"location":"minutes/19-05-07/#location-coors-tech","text":"","title":"Location: Coors Tech"},{"location":"minutes/19-05-07/#attending","text":"Jihyun Andy Thomas Antoine Bin Iga","title":"Attending"},{"location":"minutes/19-05-07/#key-takeaways","text":"The well log GAN project to be a tutorial-like paper in Geophysics Well log GAN peeps will meet weekly over the summer (TBD) In-person meetings to resume on campus in Fall semester","title":"Key takeaways"},{"location":"minutes/19-05-07/#problems","text":"Data still needs to be better cleaned Identify other potential well data types besides resistivity Check the units of the well data Hani\u2019s like a Pokemon, We gotta catch em.","title":"Problems"},{"location":"minutes/19-05-07/#updated-notes-on-well-log-paper","text":"Scope of paper Stick to CNN-based GAN Multi-dimensional training data after 1D success Tone is tutorial-like Motivation lack of geosci training data well logs are simple and high fidelity ubiquitous oil,water,mining generator no petro-phys model/simulator act as a prior for inversion mention gradients? links different types of logs well log data recovery unsupervised Contributions of paper published curated dataset published trained network GAN tutorial foothold into further GAN work Outline of paper Intro Geology of Kansas - postpone to Fall GAN background - Andy, Jihyun, Bin, and Iga RILD background - Andy Methods QC/data cleaning - Andy GAN - Andy, Jihyun, Bin, and Iga Cross-validation - Thomas spatial stats wavelet transform Fourier transform PCA Results QC/data cleaning - Andy GAN - Andy, Jihyun, Bin, and Iga Cross-validation - Thomas spatial stats wavelet transform Fourier transform PCA Discussion each result item Answer: What did/didn\u2019t the network learn speculation on use cases recommendations for changes predictions/future work Task leader breakdown Jihyun: Gan literature review Thomas: statistical analysis Andy: Data cleaning may need help Download zipped las files from KGS (link on mlgp website) check units: depth and Ohm-m Do our best to handle well geometry ID Nans Handle NaNs ID most popular aliases write functions in .py file Function 1 takes: a well log, mneumonic returns mask for nan, depth Function 2 takes: data, mask, depths returns: \u2018clean\u2019 data Interpolate over no more than 10 ft","title":"Updated notes on well log paper"},{"location":"minutes/19-05-07/#agenda-for-next-meeting","text":"Go get a burger Communicate work done over the summer Line up potential speakers from GP department for the fall","title":"Agenda for next meeting"},{"location":"minutes/19-09-25/","text":"2019 September 25 \u00b6 Location: GC 245 \u00b6 Attending : \u00b6 Jihyun Andy Khalid Key takeaways \u00b6 SEG was chock full of machine learning talks About half were interesting Many try to replace inversion with ML - naive approach Problems \u00b6 Where is everyone? Comments \u00b6 SEG talks: Hongyu: training to recover low freq seismic data from high freq Bayesian Inversion sped up by NN Uncertainty in NN output Meet separately for GAN Paper New meeting time: 10 AM, Monday? Topics of discussion for next week \u00b6 TBD","title":"2019 September 25"},{"location":"minutes/19-09-25/#2019-september-25","text":"","title":"2019 September 25"},{"location":"minutes/19-09-25/#location-gc-245","text":"","title":"Location: GC 245"},{"location":"minutes/19-09-25/#attending","text":"Jihyun Andy Khalid","title":"Attending:"},{"location":"minutes/19-09-25/#key-takeaways","text":"SEG was chock full of machine learning talks About half were interesting Many try to replace inversion with ML - naive approach","title":"Key takeaways"},{"location":"minutes/19-09-25/#problems","text":"Where is everyone?","title":"Problems"},{"location":"minutes/19-09-25/#comments","text":"SEG talks: Hongyu: training to recover low freq seismic data from high freq Bayesian Inversion sped up by NN Uncertainty in NN output Meet separately for GAN Paper New meeting time: 10 AM, Monday?","title":"Comments"},{"location":"minutes/19-09-25/#topics-of-discussion-for-next-week","text":"TBD","title":"Topics of discussion for next week"},{"location":"minutes/19-10-09/","text":"2019 May 07 \u00b6 Location: Coors Tech \u00b6 Attending \u00b6 Jihyun Andy Thomas Bin Iga Key takeaways \u00b6 We discussed the SEG abstract that interests Khalid (Wu McMechan, 2018) Novel aspect: updating weights of CNN in each iteration of inversion Abstract mixes geophysics and machine learning terminology in confusing ways Unsure how or why CNN improves inversion Problems \u00b6 Example uses a really good initial model - does the algorithm work with a poor initial model? So many layers! 26! Does the CNN need to be so big? Abstract has few details Comments \u00b6 Train a CNN to input a vector of ones and output an initial velocity model 26 hidden layers For FWI where forward modeling data given velocity model is d(v) and gradient w.r.t. weights is g, the objective function to be minimized is C(v) = ||d(v)-d_observed||^2 If CNN network is G(w)=v where w are weights of the CNN and v is the velocity model, then the objective function becomes C(w) = ||d(G(w))-d_observed||^2 Minimize the new objective function by adjusting the weights w of the CNN Paper under review Topics of discussion for next week \u00b6 Further discussion on this abstract Thomas\u2019s TasNet usage","title":"2019 May 07"},{"location":"minutes/19-10-09/#2019-may-07","text":"","title":"2019 May 07"},{"location":"minutes/19-10-09/#location-coors-tech","text":"","title":"Location: Coors Tech"},{"location":"minutes/19-10-09/#attending","text":"Jihyun Andy Thomas Bin Iga","title":"Attending"},{"location":"minutes/19-10-09/#key-takeaways","text":"We discussed the SEG abstract that interests Khalid (Wu McMechan, 2018) Novel aspect: updating weights of CNN in each iteration of inversion Abstract mixes geophysics and machine learning terminology in confusing ways Unsure how or why CNN improves inversion","title":"Key takeaways"},{"location":"minutes/19-10-09/#problems","text":"Example uses a really good initial model - does the algorithm work with a poor initial model? So many layers! 26! Does the CNN need to be so big? Abstract has few details","title":"Problems"},{"location":"minutes/19-10-09/#comments","text":"Train a CNN to input a vector of ones and output an initial velocity model 26 hidden layers For FWI where forward modeling data given velocity model is d(v) and gradient w.r.t. weights is g, the objective function to be minimized is C(v) = ||d(v)-d_observed||^2 If CNN network is G(w)=v where w are weights of the CNN and v is the velocity model, then the objective function becomes C(w) = ||d(G(w))-d_observed||^2 Minimize the new objective function by adjusting the weights w of the CNN Paper under review","title":"Comments"},{"location":"minutes/19-10-09/#topics-of-discussion-for-next-week","text":"Further discussion on this abstract Thomas\u2019s TasNet usage","title":"Topics of discussion for next week"},{"location":"projects/about/","text":"Projects \u00b6 Here we provide a series of projects we have put together! Check out each project from the drop down menu on the left and view the code from the Jupyter Notebook for that example directly on the page. Note that these Jupyter notebooks are present in the repository for you to download!","title":"Projects"},{"location":"projects/about/#projects","text":"Here we provide a series of projects we have put together! Check out each project from the drop down menu on the left and view the code from the Jupyter Notebook for that example directly on the page. Note that these Jupyter notebooks are present in the repository for you to download!","title":"Projects"},{"location":"projects/auto-encoder/ae/","text":"Autoencoders \u00b6 Autoencoders can be used to reduce the dimensionality of a dataset, cluster, denoise, interpolate, separate signals, and perform many nifty digital signal processing techniques. Examples of how to build, train, and utilize autoencoders are collected here. For now, we just have a simple 1D sequence to sequence autoencoder for denoising monochromatic signals. Autoencoding example: sine waves \u00b6 This notebook shows how to autoencode 30 Hz sine waves with varying phases. After autoencoding, the denoising properties of an autoencoder are showcased. The code is heavily commented for those just starting with keras. In [1]: import numpy as np import matplotlib.pyplot as plt from keras.models import Input , Model , load_model from keras.layers import Dense from keras.callbacks import EarlyStopping , ModelCheckpoint from keras.utils import plot_model from sklearn.cluster import KMeans from sklearn.model_selection import train_test_split from sklearn.preprocessing import scale , StandardScaler , MinMaxScaler import petname Using TensorFlow backend. In [2]: # generate training, test, and validation data n = 4096 nt = 128 f = 3.0 # frequency in Hz t = np . linspace ( 0 , 1 , nt ) # time stamps in s x = np . zeros (( n , nt )) phase = np . random . uniform ( - np . pi , np . pi , size = n ) for i in range ( n ): x [ i ,:] = np . sin ( 2 * np . pi * f * t + phase [ i ] ) In [3]: # QC generated data is phase shifted but one frequency plt . figure ( figsize = ( 8 , 2 )) for i in range ( 3 ): plt . plot ( t , x [ np . random . randint ( 0 , nt - 1 ), :]) plt . show () In [4]: # QC generated phase in [-pi,pi] plt . figure ( figsize = ( 8 , 2 )) plt . hist ( phase , bins = 31 ) plt . xlabel ( 'phase' ) plt . ylabel ( 'number of occurence' ) plt . show () In [5]: # split into test, validation, and training sets x_temp , x_test , _ , _ = train_test_split ( x , x , test_size = 0.05 ) x_train , x_valid , _ , _ = train_test_split ( x_temp , x_temp , test_size = 0.1 ) n_train = len ( x_train ) n_valid = len ( x_valid ) n_test = len ( x_test ) In [6]: # construct autoencoder network structure encoding_dim = 11 # input layer is full time series of length nt inputs = Input (( nt ,)) # add more hidden layers here encoded = Dense ( 64 , activation = 'tanh' )( inputs ) encoded = Dense ( 32 , activation = 'tanh' )( encoded ) encoded = Dense ( encoding_dim , activation = 'tanh' )( encoded ) decoded = Dense ( 32 , activation = 'tanh' )( encoded ) decoded = Dense ( 64 , activation = 'tanh' )( decoded ) # output layer is same length as input outputs = Dense ( nt , activation = 'tanh' )( decoded ) # consolidate to define autoencoder model inputs and outputs ae = Model ( inputs = inputs , outputs = outputs ) # specify encoder and decoder model for easy encoding and decoding later encoder = Model ( inputs = inputs , outputs = encoded ) encoded_input = Input ( shape = ( encoding_dim ,)) # string together decoder layers # TODO: pythonic way to do this? decoded_output = ae . layers [ - 3 ]( encoded_input ) decoded_output = ae . layers [ - 2 ]( decoded_output ) decoded_output = ae . layers [ - 1 ]( decoded_output ) decoder = Model ( inputs = encoded_input , outputs = decoded_output ) In [7]: print ( 'Full autoencoder' ) print ( ae . summary ()) print ( ' \\n Encoder portion of autoencoder' ) print ( encoder . summary ()) Full autoencoder _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 128) 0 _________________________________________________________________ dense_1 (Dense) (None, 64) 8256 _________________________________________________________________ dense_2 (Dense) (None, 32) 2080 _________________________________________________________________ dense_3 (Dense) (None, 11) 363 _________________________________________________________________ dense_4 (Dense) (None, 32) 384 _________________________________________________________________ dense_5 (Dense) (None, 64) 2112 _________________________________________________________________ dense_6 (Dense) (None, 128) 8320 ================================================================= Total params: 21,515 Trainable params: 21,515 Non-trainable params: 0 _________________________________________________________________ None Encoder portion of autoencoder _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 128) 0 _________________________________________________________________ dense_1 (Dense) (None, 64) 8256 _________________________________________________________________ dense_2 (Dense) (None, 32) 2080 _________________________________________________________________ dense_3 (Dense) (None, 11) 363 ================================================================= Total params: 10,699 Trainable params: 10,699 Non-trainable params: 0 _________________________________________________________________ None In [8]: # specify opt. strategy ae . compile ( optimizer = 'adam' , loss = 'mse' , metrics = [ 'mse' ]) In [9]: # specify training parameters and callback functions # batch size for stochastic solver batch_size = 16 # number of times entire dataset is considered in stochastic solver epochs = 100 # unique name for the network for saving unique_name = petname . name () model_filename = 'aen_sin_ %03d Hz_n= %05d _' % ( int ( f ), nt ) + unique_name + '.h5' # training history file name history_filename = 'results_' + unique_name + '.npz' # stop early after no improvement past epochs=patience and be verbose earlystopper = EarlyStopping ( patience = 100 , verbose = 1 ) # checkpoint and save model when improvement occurs checkpointer = ModelCheckpoint ( model_filename , verbose = 1 , save_best_only = True ) # consolidate callback functions for convenience callbacks = [ earlystopper , checkpointer ] In [10]: # train autoencoder results = ae . fit ( x_train , x_train , batch_size = batch_size , epochs = epochs , validation_data = ( x_valid , x_valid ), callbacks = callbacks ) Train on 3501 samples, validate on 390 samples Epoch 1/100 3501/3501 [==============================] - 1s 355us/step - loss: 0.0459 - mean_squared_error: 0.0459 - val_loss: 0.0041 - val_mean_squared_error: 0.0041 Epoch 00001: val_loss improved from inf to 0.00415, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 2/100 3501/3501 [==============================] - 1s 144us/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0015 - val_mean_squared_error: 0.0015 Epoch 00002: val_loss improved from 0.00415 to 0.00153, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 3/100 3501/3501 [==============================] - 1s 154us/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 8.3525e-04 - val_mean_squared_error: 8.3525e-04 Epoch 00003: val_loss improved from 0.00153 to 0.00084, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 4/100 3501/3501 [==============================] - 1s 145us/step - loss: 7.1871e-04 - mean_squared_error: 7.1871e-04 - val_loss: 6.5505e-04 - val_mean_squared_error: 6.5505e-04 Epoch 00004: val_loss improved from 0.00084 to 0.00066, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 5/100 3501/3501 [==============================] - 1s 148us/step - loss: 6.2149e-04 - mean_squared_error: 6.2149e-04 - val_loss: 5.7450e-04 - val_mean_squared_error: 5.7450e-04 Epoch 00005: val_loss improved from 0.00066 to 0.00057, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 6/100 3501/3501 [==============================] - 0s 142us/step - loss: 5.4520e-04 - mean_squared_error: 5.4520e-04 - val_loss: 4.9975e-04 - val_mean_squared_error: 4.9975e-04 Epoch 00006: val_loss improved from 0.00057 to 0.00050, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 7/100 3501/3501 [==============================] - 1s 144us/step - loss: 5.7018e-04 - mean_squared_error: 5.7018e-04 - val_loss: 3.9644e-04 - val_mean_squared_error: 3.9644e-04 Epoch 00007: val_loss improved from 0.00050 to 0.00040, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 8/100 3501/3501 [==============================] - 1s 143us/step - loss: 3.5918e-04 - mean_squared_error: 3.5918e-04 - val_loss: 3.1523e-04 - val_mean_squared_error: 3.1523e-04 Epoch 00008: val_loss improved from 0.00040 to 0.00032, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 9/100 3501/3501 [==============================] - 1s 148us/step - loss: 2.8607e-04 - mean_squared_error: 2.8607e-04 - val_loss: 2.6537e-04 - val_mean_squared_error: 2.6537e-04 Epoch 00009: val_loss improved from 0.00032 to 0.00027, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 10/100 3501/3501 [==============================] - 0s 140us/step - loss: 3.0162e-04 - mean_squared_error: 3.0162e-04 - val_loss: 2.1948e-04 - val_mean_squared_error: 2.1948e-04 Epoch 00010: val_loss improved from 0.00027 to 0.00022, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 11/100 3501/3501 [==============================] - 1s 153us/step - loss: 2.0970e-04 - mean_squared_error: 2.0970e-04 - val_loss: 2.5081e-04 - val_mean_squared_error: 2.5081e-04 Epoch 00011: val_loss did not improve from 0.00022 Epoch 12/100 3501/3501 [==============================] - 0s 139us/step - loss: 2.0760e-04 - mean_squared_error: 2.0760e-04 - val_loss: 1.7316e-04 - val_mean_squared_error: 1.7316e-04 Epoch 00012: val_loss improved from 0.00022 to 0.00017, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 13/100 3501/3501 [==============================] - 1s 150us/step - loss: 1.7070e-04 - mean_squared_error: 1.7070e-04 - val_loss: 1.6057e-04 - val_mean_squared_error: 1.6057e-04 Epoch 00013: val_loss improved from 0.00017 to 0.00016, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 14/100 3501/3501 [==============================] - 1s 153us/step - loss: 1.6828e-04 - mean_squared_error: 1.6828e-04 - val_loss: 1.5692e-04 - val_mean_squared_error: 1.5692e-04 Epoch 00014: val_loss improved from 0.00016 to 0.00016, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 15/100 3501/3501 [==============================] - 1s 147us/step - loss: 2.1605e-04 - mean_squared_error: 2.1605e-04 - val_loss: 1.6475e-04 - val_mean_squared_error: 1.6475e-04 Epoch 00015: val_loss did not improve from 0.00016 Epoch 16/100 3501/3501 [==============================] - 1s 145us/step - loss: 1.4884e-04 - mean_squared_error: 1.4884e-04 - val_loss: 1.3565e-04 - val_mean_squared_error: 1.3565e-04 Epoch 00016: val_loss improved from 0.00016 to 0.00014, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 17/100 3501/3501 [==============================] - 1s 144us/step - loss: 1.5588e-04 - mean_squared_error: 1.5588e-04 - val_loss: 3.6032e-04 - val_mean_squared_error: 3.6032e-04 Epoch 00017: val_loss did not improve from 0.00014 Epoch 18/100 3501/3501 [==============================] - 1s 149us/step - loss: 1.4744e-04 - mean_squared_error: 1.4744e-04 - val_loss: 1.3256e-04 - val_mean_squared_error: 1.3256e-04 Epoch 00018: val_loss improved from 0.00014 to 0.00013, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 19/100 3501/3501 [==============================] - 1s 145us/step - loss: 1.8761e-04 - mean_squared_error: 1.8761e-04 - val_loss: 1.8393e-04 - val_mean_squared_error: 1.8393e-04 Epoch 00019: val_loss did not improve from 0.00013 Epoch 20/100 3501/3501 [==============================] - 1s 151us/step - loss: 1.2678e-04 - mean_squared_error: 1.2678e-04 - val_loss: 9.2544e-05 - val_mean_squared_error: 9.2544e-05 Epoch 00020: val_loss improved from 0.00013 to 0.00009, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 21/100 3501/3501 [==============================] - 1s 155us/step - loss: 1.1560e-04 - mean_squared_error: 1.1560e-04 - val_loss: 1.2224e-04 - val_mean_squared_error: 1.2224e-04 Epoch 00021: val_loss did not improve from 0.00009 Epoch 22/100 3501/3501 [==============================] - 1s 152us/step - loss: 1.1742e-04 - mean_squared_error: 1.1742e-04 - val_loss: 1.2506e-04 - val_mean_squared_error: 1.2506e-04 Epoch 00022: val_loss did not improve from 0.00009 Epoch 23/100 3501/3501 [==============================] - 1s 150us/step - loss: 1.2605e-04 - mean_squared_error: 1.2605e-04 - val_loss: 9.8797e-05 - val_mean_squared_error: 9.8797e-05 Epoch 00023: val_loss did not improve from 0.00009 Epoch 24/100 3501/3501 [==============================] - 1s 149us/step - loss: 2.1077e-04 - mean_squared_error: 2.1077e-04 - val_loss: 2.1052e-04 - val_mean_squared_error: 2.1052e-04 Epoch 00024: val_loss did not improve from 0.00009 Epoch 25/100 3501/3501 [==============================] - 1s 147us/step - loss: 1.5182e-04 - mean_squared_error: 1.5182e-04 - val_loss: 6.7037e-05 - val_mean_squared_error: 6.7037e-05 Epoch 00025: val_loss improved from 0.00009 to 0.00007, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 26/100 3501/3501 [==============================] - 1s 145us/step - loss: 6.6275e-05 - mean_squared_error: 6.6275e-05 - val_loss: 6.4966e-05 - val_mean_squared_error: 6.4966e-05 Epoch 00026: val_loss improved from 0.00007 to 0.00006, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 27/100 3501/3501 [==============================] - 0s 141us/step - loss: 6.3563e-05 - mean_squared_error: 6.3563e-05 - val_loss: 6.3247e-05 - val_mean_squared_error: 6.3247e-05 Epoch 00027: val_loss improved from 0.00006 to 0.00006, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 28/100 3501/3501 [==============================] - 0s 140us/step - loss: 6.5669e-05 - mean_squared_error: 6.5669e-05 - val_loss: 7.6097e-05 - val_mean_squared_error: 7.6097e-05 Epoch 00028: val_loss did not improve from 0.00006 Epoch 29/100 3501/3501 [==============================] - 0s 140us/step - loss: 1.8151e-04 - mean_squared_error: 1.8151e-04 - val_loss: 7.0479e-04 - val_mean_squared_error: 7.0479e-04 Epoch 00029: val_loss did not improve from 0.00006 Epoch 30/100 3501/3501 [==============================] - 1s 146us/step - loss: 4.1471e-04 - mean_squared_error: 4.1471e-04 - val_loss: 5.4083e-05 - val_mean_squared_error: 5.4083e-05 Epoch 00030: val_loss improved from 0.00006 to 0.00005, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 31/100 3501/3501 [==============================] - 1s 146us/step - loss: 5.3065e-05 - mean_squared_error: 5.3065e-05 - val_loss: 5.1097e-05 - val_mean_squared_error: 5.1097e-05 Epoch 00031: val_loss improved from 0.00005 to 0.00005, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 32/100 3501/3501 [==============================] - 1s 168us/step - loss: 5.1508e-05 - mean_squared_error: 5.1508e-05 - val_loss: 4.9787e-05 - val_mean_squared_error: 4.9787e-05 Epoch 00032: val_loss improved from 0.00005 to 0.00005, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 33/100 3501/3501 [==============================] - 1s 176us/step - loss: 5.0157e-05 - mean_squared_error: 5.0157e-05 - val_loss: 4.8304e-05 - val_mean_squared_error: 4.8304e-05 Epoch 00033: val_loss improved from 0.00005 to 0.00005, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 34/100 3501/3501 [==============================] - 0s 141us/step - loss: 6.3884e-05 - mean_squared_error: 6.3884e-05 - val_loss: 1.5991e-04 - val_mean_squared_error: 1.5991e-04 Epoch 00034: val_loss did not improve from 0.00005 Epoch 35/100 3501/3501 [==============================] - 1s 235us/step - loss: 8.3365e-05 - mean_squared_error: 8.3365e-05 - val_loss: 6.0017e-05 - val_mean_squared_error: 6.0017e-05 Epoch 00035: val_loss did not improve from 0.00005 Epoch 36/100 3501/3501 [==============================] - 1s 281us/step - loss: 5.7621e-05 - mean_squared_error: 5.7621e-05 - val_loss: 4.7106e-05 - val_mean_squared_error: 4.7106e-05 Epoch 00036: val_loss improved from 0.00005 to 0.00005, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 37/100 3501/3501 [==============================] - 1s 172us/step - loss: 5.5007e-05 - mean_squared_error: 5.5007e-05 - val_loss: 5.9474e-05 - val_mean_squared_error: 5.9474e-05 Epoch 00037: val_loss did not improve from 0.00005 Epoch 38/100 3501/3501 [==============================] - 1s 200us/step - loss: 3.2148e-04 - mean_squared_error: 3.2148e-04 - val_loss: 6.6656e-04 - val_mean_squared_error: 6.6656e-04 Epoch 00038: val_loss did not improve from 0.00005 Epoch 39/100 3501/3501 [==============================] - 1s 148us/step - loss: 1.5987e-04 - mean_squared_error: 1.5987e-04 - val_loss: 4.3525e-05 - val_mean_squared_error: 4.3525e-05 Epoch 00039: val_loss improved from 0.00005 to 0.00004, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 40/100 3501/3501 [==============================] - 1s 143us/step - loss: 4.0375e-05 - mean_squared_error: 4.0375e-05 - val_loss: 3.8799e-05 - val_mean_squared_error: 3.8799e-05 Epoch 00040: val_loss improved from 0.00004 to 0.00004, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 41/100 3501/3501 [==============================] - 1s 173us/step - loss: 3.8916e-05 - mean_squared_error: 3.8916e-05 - val_loss: 3.7477e-05 - val_mean_squared_error: 3.7477e-05 Epoch 00041: val_loss improved from 0.00004 to 0.00004, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 42/100 3501/3501 [==============================] - 0s 142us/step - loss: 3.8079e-05 - mean_squared_error: 3.8079e-05 - val_loss: 3.7386e-05 - val_mean_squared_error: 3.7386e-05 Epoch 00042: val_loss improved from 0.00004 to 0.00004, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 43/100 3501/3501 [==============================] - 1s 181us/step - loss: 3.8363e-05 - mean_squared_error: 3.8363e-05 - val_loss: 4.6886e-05 - val_mean_squared_error: 4.6886e-05 Epoch 00043: val_loss did not improve from 0.00004 Epoch 44/100 3501/3501 [==============================] - 1s 207us/step - loss: 9.3752e-05 - mean_squared_error: 9.3752e-05 - val_loss: 8.5156e-05 - val_mean_squared_error: 8.5156e-05 Epoch 00044: val_loss did not improve from 0.00004 Epoch 45/100 3501/3501 [==============================] - 1s 234us/step - loss: 1.9868e-04 - mean_squared_error: 1.9868e-04 - val_loss: 4.4134e-05 - val_mean_squared_error: 4.4134e-05 Epoch 00045: val_loss did not improve from 0.00004 Epoch 46/100 3501/3501 [==============================] - 1s 245us/step - loss: 3.5457e-05 - mean_squared_error: 3.5457e-05 - val_loss: 3.3888e-05 - val_mean_squared_error: 3.3888e-05 Epoch 00046: val_loss improved from 0.00004 to 0.00003, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 47/100 3501/3501 [==============================] - 1s 244us/step - loss: 3.5797e-05 - mean_squared_error: 3.5797e-05 - val_loss: 4.5024e-05 - val_mean_squared_error: 4.5024e-05 Epoch 00047: val_loss did not improve from 0.00003 Epoch 48/100 3501/3501 [==============================] - 1s 150us/step - loss: 3.4105e-05 - mean_squared_error: 3.4105e-05 - val_loss: 3.1702e-05 - val_mean_squared_error: 3.1702e-05 Epoch 00048: val_loss improved from 0.00003 to 0.00003, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 49/100 3501/3501 [==============================] - 1s 162us/step - loss: 3.0570e-04 - mean_squared_error: 3.0570e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013 Epoch 00049: val_loss did not improve from 0.00003 Epoch 50/100 3501/3501 [==============================] - 1s 213us/step - loss: 1.1956e-04 - mean_squared_error: 1.1956e-04 - val_loss: 3.1140e-05 - val_mean_squared_error: 3.1140e-05 Epoch 00050: val_loss improved from 0.00003 to 0.00003, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 51/100 3501/3501 [==============================] - 1s 159us/step - loss: 3.1376e-05 - mean_squared_error: 3.1376e-05 - val_loss: 3.0275e-05 - val_mean_squared_error: 3.0275e-05 Epoch 00051: val_loss improved from 0.00003 to 0.00003, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 52/100 3501/3501 [==============================] - 1s 150us/step - loss: 4.8290e-05 - mean_squared_error: 4.8290e-05 - val_loss: 1.2244e-04 - val_mean_squared_error: 1.2244e-04 Epoch 00052: val_loss did not improve from 0.00003 Epoch 53/100 3501/3501 [==============================] - 1s 158us/step - loss: 4.6348e-05 - mean_squared_error: 4.6348e-05 - val_loss: 3.0070e-05 - val_mean_squared_error: 3.0070e-05 Epoch 00053: val_loss improved from 0.00003 to 0.00003, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 54/100 3501/3501 [==============================] - 1s 144us/step - loss: 4.4264e-05 - mean_squared_error: 4.4264e-05 - val_loss: 3.8164e-05 - val_mean_squared_error: 3.8164e-05 Epoch 00054: val_loss did not improve from 0.00003 Epoch 55/100 3501/3501 [==============================] - 1s 147us/step - loss: 4.0062e-05 - mean_squared_error: 4.0062e-05 - val_loss: 7.6668e-05 - val_mean_squared_error: 7.6668e-05 Epoch 00055: val_loss did not improve from 0.00003 Epoch 56/100 3501/3501 [==============================] - 1s 150us/step - loss: 8.1536e-05 - mean_squared_error: 8.1536e-05 - val_loss: 3.6079e-05 - val_mean_squared_error: 3.6079e-05 Epoch 00056: val_loss did not improve from 0.00003 Epoch 57/100 3501/3501 [==============================] - 1s 146us/step - loss: 1.0047e-04 - mean_squared_error: 1.0047e-04 - val_loss: 2.0018e-04 - val_mean_squared_error: 2.0018e-04 Epoch 00057: val_loss did not improve from 0.00003 Epoch 58/100 3501/3501 [==============================] - 1s 212us/step - loss: 1.0954e-04 - mean_squared_error: 1.0954e-04 - val_loss: 3.3508e-05 - val_mean_squared_error: 3.3508e-05 Epoch 00058: val_loss did not improve from 0.00003 Epoch 59/100 3501/3501 [==============================] - 1s 197us/step - loss: 3.8543e-05 - mean_squared_error: 3.8543e-05 - val_loss: 2.6846e-05 - val_mean_squared_error: 2.6846e-05 Epoch 00059: val_loss improved from 0.00003 to 0.00003, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 60/100 3501/3501 [==============================] - 1s 169us/step - loss: 3.0555e-05 - mean_squared_error: 3.0555e-05 - val_loss: 4.8613e-05 - val_mean_squared_error: 4.8613e-05 Epoch 00060: val_loss did not improve from 0.00003 Epoch 61/100 3501/3501 [==============================] - 1s 146us/step - loss: 2.5328e-04 - mean_squared_error: 2.5328e-04 - val_loss: 4.4946e-04 - val_mean_squared_error: 4.4946e-04 Epoch 00061: val_loss did not improve from 0.00003 Epoch 62/100 3501/3501 [==============================] - 1s 147us/step - loss: 7.1338e-05 - mean_squared_error: 7.1338e-05 - val_loss: 2.5187e-05 - val_mean_squared_error: 2.5187e-05 Epoch 00062: val_loss improved from 0.00003 to 0.00003, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 63/100 3501/3501 [==============================] - 1s 217us/step - loss: 2.6905e-05 - mean_squared_error: 2.6905e-05 - val_loss: 3.0878e-05 - val_mean_squared_error: 3.0878e-05 Epoch 00063: val_loss did not improve from 0.00003 Epoch 64/100 3501/3501 [==============================] - 1s 266us/step - loss: 2.6612e-05 - mean_squared_error: 2.6612e-05 - val_loss: 2.9492e-05 - val_mean_squared_error: 2.9492e-05 Epoch 00064: val_loss did not improve from 0.00003 Epoch 65/100 3501/3501 [==============================] - 1s 167us/step - loss: 7.0185e-05 - mean_squared_error: 7.0185e-05 - val_loss: 3.9116e-05 - val_mean_squared_error: 3.9116e-05 Epoch 00065: val_loss did not improve from 0.00003 Epoch 66/100 3501/3501 [==============================] - 0s 140us/step - loss: 8.2021e-05 - mean_squared_error: 8.2021e-05 - val_loss: 2.5925e-04 - val_mean_squared_error: 2.5925e-04 Epoch 00066: val_loss did not improve from 0.00003 Epoch 67/100 3501/3501 [==============================] - 1s 190us/step - loss: 7.5346e-05 - mean_squared_error: 7.5346e-05 - val_loss: 4.5074e-05 - val_mean_squared_error: 4.5074e-05 Epoch 00067: val_loss did not improve from 0.00003 Epoch 68/100 3501/3501 [==============================] - 1s 188us/step - loss: 3.0216e-05 - mean_squared_error: 3.0216e-05 - val_loss: 2.6978e-05 - val_mean_squared_error: 2.6978e-05 Epoch 00068: val_loss did not improve from 0.00003 Epoch 69/100 3501/3501 [==============================] - 1s 163us/step - loss: 1.0620e-04 - mean_squared_error: 1.0620e-04 - val_loss: 4.7271e-05 - val_mean_squared_error: 4.7271e-05 Epoch 00069: val_loss did not improve from 0.00003 Epoch 70/100 3501/3501 [==============================] - 1s 195us/step - loss: 2.8511e-04 - mean_squared_error: 2.8511e-04 - val_loss: 8.3107e-05 - val_mean_squared_error: 8.3107e-05 Epoch 00070: val_loss did not improve from 0.00003 Epoch 71/100 3501/3501 [==============================] - 1s 199us/step - loss: 3.4888e-05 - mean_squared_error: 3.4888e-05 - val_loss: 2.1939e-05 - val_mean_squared_error: 2.1939e-05 Epoch 00071: val_loss improved from 0.00003 to 0.00002, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 72/100 3501/3501 [==============================] - 1s 151us/step - loss: 2.2851e-05 - mean_squared_error: 2.2851e-05 - val_loss: 2.8569e-05 - val_mean_squared_error: 2.8569e-05 Epoch 00072: val_loss did not improve from 0.00002 Epoch 73/100 3501/3501 [==============================] - 1s 153us/step - loss: 2.3349e-05 - mean_squared_error: 2.3349e-05 - val_loss: 2.2561e-05 - val_mean_squared_error: 2.2561e-05 Epoch 00073: val_loss did not improve from 0.00002 Epoch 74/100 3501/3501 [==============================] - 0s 138us/step - loss: 3.5593e-05 - mean_squared_error: 3.5593e-05 - val_loss: 4.1796e-05 - val_mean_squared_error: 4.1796e-05 Epoch 00074: val_loss did not improve from 0.00002 Epoch 75/100 3501/3501 [==============================] - 1s 147us/step - loss: 3.6719e-05 - mean_squared_error: 3.6719e-05 - val_loss: 8.7785e-05 - val_mean_squared_error: 8.7785e-05 Epoch 00075: val_loss did not improve from 0.00002 Epoch 76/100 3501/3501 [==============================] - 0s 140us/step - loss: 1.3555e-04 - mean_squared_error: 1.3555e-04 - val_loss: 2.8934e-05 - val_mean_squared_error: 2.8934e-05 Epoch 00076: val_loss did not improve from 0.00002 Epoch 77/100 3501/3501 [==============================] - 1s 149us/step - loss: 2.8458e-05 - mean_squared_error: 2.8458e-05 - val_loss: 3.1365e-05 - val_mean_squared_error: 3.1365e-05 Epoch 00077: val_loss did not improve from 0.00002 Epoch 78/100 3501/3501 [==============================] - 1s 144us/step - loss: 5.7461e-05 - mean_squared_error: 5.7461e-05 - val_loss: 3.6815e-05 - val_mean_squared_error: 3.6815e-05 Epoch 00078: val_loss did not improve from 0.00002 Epoch 79/100 3501/3501 [==============================] - 1s 148us/step - loss: 6.4324e-05 - mean_squared_error: 6.4324e-05 - val_loss: 8.9769e-05 - val_mean_squared_error: 8.9769e-05 Epoch 00079: val_loss did not improve from 0.00002 Epoch 80/100 3501/3501 [==============================] - 1s 150us/step - loss: 5.9422e-05 - mean_squared_error: 5.9422e-05 - val_loss: 9.6168e-05 - val_mean_squared_error: 9.6168e-05 Epoch 00080: val_loss did not improve from 0.00002 Epoch 81/100 3501/3501 [==============================] - 1s 144us/step - loss: 2.4627e-04 - mean_squared_error: 2.4627e-04 - val_loss: 6.1650e-04 - val_mean_squared_error: 6.1650e-04 Epoch 00081: val_loss did not improve from 0.00002 Epoch 82/100 3501/3501 [==============================] - 0s 139us/step - loss: 6.7262e-05 - mean_squared_error: 6.7262e-05 - val_loss: 2.0998e-05 - val_mean_squared_error: 2.0998e-05 Epoch 00082: val_loss improved from 0.00002 to 0.00002, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 83/100 3501/3501 [==============================] - 1s 144us/step - loss: 2.1539e-05 - mean_squared_error: 2.1539e-05 - val_loss: 4.2927e-05 - val_mean_squared_error: 4.2927e-05 Epoch 00083: val_loss did not improve from 0.00002 Epoch 84/100 3501/3501 [==============================] - 1s 150us/step - loss: 6.0519e-05 - mean_squared_error: 6.0519e-05 - val_loss: 3.9719e-05 - val_mean_squared_error: 3.9719e-05 Epoch 00084: val_loss did not improve from 0.00002 Epoch 85/100 3501/3501 [==============================] - 1s 146us/step - loss: 2.9836e-05 - mean_squared_error: 2.9836e-05 - val_loss: 2.8181e-05 - val_mean_squared_error: 2.8181e-05 Epoch 00085: val_loss did not improve from 0.00002 Epoch 86/100 3501/3501 [==============================] - 0s 141us/step - loss: 5.5841e-05 - mean_squared_error: 5.5841e-05 - val_loss: 2.7892e-05 - val_mean_squared_error: 2.7892e-05 Epoch 00086: val_loss did not improve from 0.00002 Epoch 87/100 3501/3501 [==============================] - 0s 140us/step - loss: 4.5011e-05 - mean_squared_error: 4.5011e-05 - val_loss: 8.4272e-05 - val_mean_squared_error: 8.4272e-05 Epoch 00087: val_loss did not improve from 0.00002 Epoch 88/100 3501/3501 [==============================] - 1s 152us/step - loss: 9.3875e-05 - mean_squared_error: 9.3875e-05 - val_loss: 3.2714e-04 - val_mean_squared_error: 3.2714e-04 Epoch 00088: val_loss did not improve from 0.00002 Epoch 89/100 3501/3501 [==============================] - 1s 147us/step - loss: 1.0599e-04 - mean_squared_error: 1.0599e-04 - val_loss: 2.5802e-05 - val_mean_squared_error: 2.5802e-05 Epoch 00089: val_loss did not improve from 0.00002 Epoch 90/100 3501/3501 [==============================] - 1s 146us/step - loss: 2.1064e-05 - mean_squared_error: 2.1064e-05 - val_loss: 2.1859e-05 - val_mean_squared_error: 2.1859e-05 Epoch 00090: val_loss did not improve from 0.00002 Epoch 91/100 3501/3501 [==============================] - 1s 155us/step - loss: 2.4710e-05 - mean_squared_error: 2.4710e-05 - val_loss: 3.3201e-05 - val_mean_squared_error: 3.3201e-05 Epoch 00091: val_loss did not improve from 0.00002 Epoch 92/100 3501/3501 [==============================] - 1s 143us/step - loss: 1.9832e-04 - mean_squared_error: 1.9832e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012 Epoch 00092: val_loss did not improve from 0.00002 Epoch 93/100 3501/3501 [==============================] - 0s 138us/step - loss: 1.8404e-04 - mean_squared_error: 1.8404e-04 - val_loss: 1.9404e-05 - val_mean_squared_error: 1.9404e-05 Epoch 00093: val_loss improved from 0.00002 to 0.00002, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 94/100 3501/3501 [==============================] - 0s 141us/step - loss: 1.7544e-05 - mean_squared_error: 1.7544e-05 - val_loss: 1.6940e-05 - val_mean_squared_error: 1.6940e-05 Epoch 00094: val_loss improved from 0.00002 to 0.00002, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 95/100 3501/3501 [==============================] - 0s 139us/step - loss: 1.7011e-05 - mean_squared_error: 1.7011e-05 - val_loss: 1.6420e-05 - val_mean_squared_error: 1.6420e-05 Epoch 00095: val_loss improved from 0.00002 to 0.00002, saving model to aen_sin_003Hz_n=00128_rodent.h5 Epoch 96/100 3501/3501 [==============================] - 1s 145us/step - loss: 1.6887e-05 - mean_squared_error: 1.6887e-05 - val_loss: 1.6874e-05 - val_mean_squared_error: 1.6874e-05 Epoch 00096: val_loss did not improve from 0.00002 Epoch 97/100 3501/3501 [==============================] - 0s 143us/step - loss: 1.7039e-05 - mean_squared_error: 1.7039e-05 - val_loss: 2.4081e-05 - val_mean_squared_error: 2.4081e-05 Epoch 00097: val_loss did not improve from 0.00002 Epoch 98/100 3501/3501 [==============================] - 1s 149us/step - loss: 1.3684e-04 - mean_squared_error: 1.3684e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012 Epoch 00098: val_loss did not improve from 0.00002 Epoch 99/100 3501/3501 [==============================] - 1s 147us/step - loss: 2.4728e-04 - mean_squared_error: 2.4728e-04 - val_loss: 1.8387e-05 - val_mean_squared_error: 1.8387e-05 Epoch 00099: val_loss did not improve from 0.00002 Epoch 100/100 3501/3501 [==============================] - 1s 145us/step - loss: 1.6816e-05 - mean_squared_error: 1.6816e-05 - val_loss: 1.5990e-05 - val_mean_squared_error: 1.5990e-05 Epoch 00100: val_loss improved from 0.00002 to 0.00002, saving model to aen_sin_003Hz_n=00128_rodent.h5 In [11]: # QC training and validation curves (should follow eachother) plt . figure ( figsize = ( 8 , 2 )) plt . plot ( results . history [ 'val_loss' ], label = 'val' ) plt . plot ( results . history [ 'loss' ], label = 'train' ) plt . xlabel ( 'epoch index' ) plt . ylabel ( 'loss value (MSE)' ) plt . legend () plt . show () In [12]: # QC that autoencoder can autoencode a sine wave from the test set decoded_sin = ae . predict ( x_test ) plt . figure ( figsize = ( 8 , 2 )) plt . plot ( t , decoded_sin [ np . random . randint ( 0 , n_test - 1 ),:]) plt . xlabel ( 'time (s)' ) plt . ylabel ( 'amplitude' ) plt . show () Inspect latent space \u00b6 What does the encoded representation of our sine wave look like? What does the decoded version of each latent dimension look like? In [13]: # encoded representation of the test set encoded_test = encoder . predict ( x_test ) # decoded representation of each dimension of the latent space decoded_latent = decoder . predict ( np . eye ( 11 ) ) In [14]: # QC each latent space dimension distribution and corresponding decoded representation cmap = plt . cm . get_cmap ( 'Dark2' , 11 ) plt . figure ( figsize = ( 10 , 5 )) plt . subplot ( 121 ) violins = plt . violinplot ( encoded_test , vert = False ) for i , violin in enumerate ( violins [ 'bodies' ]): violin . set_color ( cmap ( i )) plt . title ( 'Distribution of latent space values' ) plt . xlabel ( 'latent space value' ) plt . ylabel ( 'latent space dimension index' ) plt . subplot ( 122 ) for i in range ( 11 ): plt . plot ( t , decoded_latent [ i ,:] * 0.5 + i , c = cmap ( i )) plt . title ( 'Decoded latent space dimensions' ) plt . xlabel ( 'time (s)' ) plt . gca () . yaxis . set_ticklabels ([]) plt . show () Showcase denoising properties of autoencoder \u00b6 Here is where the fun begins with denoising using autoencoders. Let's add Gaussian noise with standard devations between one and seven then observe how well the autoencoder denoises. Note that we have basically trained nonlinear combination of connected numbers (a network) that collectively excel at encoding and decoding 30 Hz sine waves. So, we expect a 30 Hz sine wave upon output for a wide variety of inputs through this network. Recall that the input signal is a sine wave with a peak-to-peak amplitude of 2 - our noise levels are considerably high here. (We could improve denoising by including noisy sine waves in our training data) In [15]: # specific sine wave index of interest sin_index = np . random . randint ( 0 , n_test - 1 ) In [17]: # plot effects of noise on input slope = 3 fcos = 10 for i in range ( 7 ): plt . figure ( figsize = ( 8 , 2 )) sin_noisy = x_test + np . random . randn ( n_test , nt ) * i #sin_noisy = x_test + i*np.cos(2*np.pi*np.linspace(0,1,128)*2) #sin_noisy = x_test + np.random.randn(n_test,nt)*i + np.cos(2*np.pi*np.linspace(0,1,128)*fcos) decoded_sin = ae . predict ( sin_noisy ) plt . plot ( t , x_test [ sin_index ,:], 'k' , lw = 10 , alpha = 0.2 , label = 'no noise' ) plt . plot ( t , sin_noisy [ sin_index ,:], label = 'noisy input' ) plt . plot ( t , decoded_sin [ sin_index ,:], label = 'denoised' ) plt . title ( 'noise $\\sigma$ = %2.1f ' % i ) plt . legend ( loc = 'upper left' ) plt . show () In [ ]:","title":"Autoencoders"},{"location":"projects/auto-encoder/ae/#autoencoders","text":"Autoencoders can be used to reduce the dimensionality of a dataset, cluster, denoise, interpolate, separate signals, and perform many nifty digital signal processing techniques. Examples of how to build, train, and utilize autoencoders are collected here. For now, we just have a simple 1D sequence to sequence autoencoder for denoising monochromatic signals.","title":"Autoencoders"},{"location":"projects/auto-encoder/vae/","text":"Variational Autoencoders \u00b6 Variational autoencoders view autoencoding from a statistical perspective. Like classical autoencoders, they encode a dataset into a lower dimensional latent space. Additionally, though, variational autoencoders constrain the encoded vectors to roughly follow a probability distribution, e.g. a normal distribution. Here\u2019s an example of a variational autoencoder for the same 1D sequence to sequence monochromatic signal encoding problem. Variational Autoencoder \u00b6 Much of this code is from https://blog.keras.io/building-autoencoders-in-keras.html and https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py Tutorials can be found at https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf , https://jaan.io/what-is-variational-autoencoder-vae-tutorial/ , and https://arxiv.org/abs/1606.05908 This notebook uses the same toy problem as the autoencoding notebook. Here we demonstrate the use of a variational autoencoder. In [1]: import numpy as np import matplotlib.pyplot as plt import scipy.stats from keras.models import Input , Model , load_model from keras.layers import Dense , LeakyReLU , Lambda from keras.callbacks import EarlyStopping , ModelCheckpoint from keras.utils import plot_model from keras.losses import mse from keras import backend as K from sklearn.model_selection import train_test_split import petname Using TensorFlow backend. In [2]: # generate training, test, and validation data n = 4096 nt = 128 f = 3.0 # frequency in Hz t = np . linspace ( 0 , 1 , nt ) # time stamps in s x = np . zeros (( n , nt )) phase = np . random . uniform ( - np . pi , np . pi , size = n ) for i in range ( n ): x [ i ,:] = np . sin ( 2 * np . pi * f * t + phase [ i ] ) In [3]: # QC generated data is phase shifted but one frequency plt . figure ( figsize = ( 8 , 2 )) for i in range ( 3 ): plt . plot ( t , x [ np . random . randint ( 0 , nt - 1 ), :]) plt . show () In [4]: # QC generated phase in [-pi,pi] plt . figure ( figsize = ( 8 , 2 )) plt . hist ( phase , bins = 31 ) plt . xlabel ( 'phase' ) plt . ylabel ( 'number of occurence' ) plt . show () In [5]: # split into test, validation, and training sets x_temp , x_test , _ , _ = train_test_split ( x , x , test_size = 0.05 ) x_train , x_valid , _ , _ = train_test_split ( x_temp , x_temp , test_size = 0.1 ) n_train = len ( x_train ) n_valid = len ( x_valid ) n_test = len ( x_test ) In [6]: # specify training parameters and callback functions # batch size for stochastic solver batch_size = 16 # number of times entire dataset is considered in stochastic solver epochs = 100 # unique name for the network for saving unique_name = petname . name () model_filename = 'aen_sin_ %03d Hz_n= %05d _' % ( int ( f ), nt ) + unique_name + '.h5' # training history file name history_filename = 'results_' + unique_name + '.npz' # stop early after no improvement past epochs=patience and be verbose earlystopper = EarlyStopping ( patience = 100 , verbose = 1 ) # checkpoint and save model when improvement occurs checkpointer = ModelCheckpoint ( model_filename , verbose = 1 , save_best_only = True ) # consolidate callback functions for convenience callbacks = [ earlystopper , checkpointer ] Now things get a bit different from a vanilla autoencoder. First, we set the dimensions of the latent space. For this example we can get away with only one dimension. Intuitively, since the only difference between training examples is the phase, we only need one to encode one dimension. In [7]: # encoding dimension; i.e. dimensionality of the latent space encoding_dim = 1 Next we define a function to draw samples from a Gaussian, given the mean and standard deviation. We sample to encode in the latent space. Further, the way this function is defined, it lets us use backpropagation on the mean and standard deviation, even though there's a probabilistic element to this operation (this is the \"reparameterization trick\"). In [8]: # define a function to sample from gaussian, given mean and log variance def sampling ( args ): z_mean , z_log_sigma = args epsilon = K . random_normal ( shape = ( encoding_dim ,)) return z_mean + K . exp ( z_log_sigma ) * epsilon Network structure is similar to the autoencoder. The main difference is in the middle of the network: z_mean and z_log_sigma. These layers encode a mean and log(std) that determine the pdf that we draw the encoding in the latent space from. The \"Lambda\" layer then draws a sample from that pdf, and z is the encoded signal in the latent space. In [9]: # construct variational autoencoder network structure # input layer is full time series of length nt inputs = Input (( nt ,)) # encoder hidden layers encoded = Dense ( 64 )( inputs ) encoded = LeakyReLU ( alpha = 0.2 )( encoded ) encoded = Dense ( 32 )( encoded ) encoded = LeakyReLU ( alpha = 0.2 )( encoded ) z_mean = Dense ( encoding_dim )( encoded ) z_log_sigma = Dense ( encoding_dim )( encoded ) z = Lambda ( sampling , output_shape = ( encoding_dim ,))([ z_mean , z_log_sigma ]) # decoder hidden layers # explicitly named so we can define the decoder model #decoder_a = Dense(32) #decoder_b = Dense(64) #outputter = Dense(nt,activation='tanh') decoded = Dense ( 32 )( z ) decoded = LeakyReLU ( alpha = 0.2 )( decoded ) decoded = Dense ( 64 )( decoded ) decoded = LeakyReLU ( alpha = 0.2 )( decoded ) # output layer is same length as input outputs = Dense ( nt , activation = 'tanh' )( decoded ) # consolidate to define autoencoder model inputs and outputs vae = Model ( inputs = inputs , outputs = outputs ) # specify encoder and decoder model for easy encoding and decoding later encoder = Model ( inputs = inputs , outputs = [ z_mean , z_log_sigma , z ], name = 'encoder' ) # create a placeholder for an encoded input encoded_input = Input ( shape = ( encoding_dim ,)) # retrieve the last layers of the autoencoder model decoded_output = vae . layers [ - 5 ]( encoded_input ) decoded_output = vae . layers [ - 4 ]( decoded_output ) decoded_output = vae . layers [ - 3 ]( decoded_output ) decoded_output = vae . layers [ - 2 ]( decoded_output ) decoded_output = vae . layers [ - 1 ]( decoded_output ) # create the decoder model decoder = Model ( inputs = encoded_input , outputs = decoded_output , name = 'decoder' ) In [10]: print ( 'Full autoencoder' ) print ( vae . summary ()) #print('\\n Encoder portion of autoencoder') #print(vae_encoder.summary()) #print('\\n Decoder portion of autoencoder') #print(vae_decoder.summary()) Full autoencoder __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) (None, 128) 0 __________________________________________________________________________________________________ dense_1 (Dense) (None, 64) 8256 input_1[0][0] __________________________________________________________________________________________________ leaky_re_lu_1 (LeakyReLU) (None, 64) 0 dense_1[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 32) 2080 leaky_re_lu_1[0][0] __________________________________________________________________________________________________ leaky_re_lu_2 (LeakyReLU) (None, 32) 0 dense_2[0][0] __________________________________________________________________________________________________ dense_3 (Dense) (None, 1) 33 leaky_re_lu_2[0][0] __________________________________________________________________________________________________ dense_4 (Dense) (None, 1) 33 leaky_re_lu_2[0][0] __________________________________________________________________________________________________ lambda_1 (Lambda) (None, 1) 0 dense_3[0][0] dense_4[0][0] __________________________________________________________________________________________________ dense_5 (Dense) (None, 32) 64 lambda_1[0][0] __________________________________________________________________________________________________ leaky_re_lu_3 (LeakyReLU) (None, 32) 0 dense_5[0][0] __________________________________________________________________________________________________ dense_6 (Dense) (None, 64) 2112 leaky_re_lu_3[0][0] __________________________________________________________________________________________________ leaky_re_lu_4 (LeakyReLU) (None, 64) 0 dense_6[0][0] __________________________________________________________________________________________________ dense_7 (Dense) (None, 128) 8320 leaky_re_lu_4[0][0] ================================================================================================== Total params: 20,898 Trainable params: 20,898 Non-trainable params: 0 __________________________________________________________________________________________________ None The loss function is another key difference between standard autoencoders and variational autoencoders. A standard autoencoder simply minimizes reconstruction loss. A variational autoencoder minimizes both reconstruction loss and the KL divergence. The KL divergence is a measure of how much two probability distributions differ. Minimizing the KL divergence here means that we are encouraging the latent space encodings to have a normal distribution. The regularization parameter balances between reconstruction loss and enforcing a normal distribution in the latent space. In [11]: # specify loss # regularization balances signal reconstruction with # a Gaussian distribution in the latent space regularization = 10 def vae_loss ( input_img , output ): # compute the average MSE error, then scale it up, ie. simply sum on all axes reconstruction_loss = K . sum ( K . square ( output - input_img )) kl_loss = - 0.5 * K . sum ( 1 + z_log_sigma - K . square ( z_mean ) - K . square ( K . exp ( z_log_sigma )), axis =- 1 ) # return the average loss over all images in batch total_loss = K . mean ( reconstruction_loss + regularization * kl_loss ) return total_loss vae . compile ( optimizer = 'adam' , loss = vae_loss , metrics = [ 'mse' ]) In [12]: # train variational autoencoder results = vae . fit ( x_train , x_train , shuffle = True , batch_size = batch_size , epochs = epochs , validation_data = ( x_valid , x_valid ), callbacks = callbacks ) Train on 3501 samples, validate on 390 samples Epoch 1/100 3501/3501 [==============================] - 1s 213us/step - loss: 572.6987 - mean_squared_error: 0.2705 - val_loss: 494.7158 - val_mean_squared_error: 0.2357 Epoch 00001: val_loss improved from inf to 494.71583, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 2/100 3501/3501 [==============================] - 0s 86us/step - loss: 375.1624 - mean_squared_error: 0.1734 - val_loss: 239.9433 - val_mean_squared_error: 0.1059 Epoch 00002: val_loss improved from 494.71583 to 239.94333, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 3/100 3501/3501 [==============================] - 0s 85us/step - loss: 188.5630 - mean_squared_error: 0.0773 - val_loss: 196.9777 - val_mean_squared_error: 0.0851 Epoch 00003: val_loss improved from 239.94333 to 196.97767, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 4/100 3501/3501 [==============================] - 0s 85us/step - loss: 150.1980 - mean_squared_error: 0.0625 - val_loss: 122.2335 - val_mean_squared_error: 0.0504 Epoch 00004: val_loss improved from 196.97767 to 122.23345, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 5/100 3501/3501 [==============================] - 0s 95us/step - loss: 107.6601 - mean_squared_error: 0.0433 - val_loss: 83.0565 - val_mean_squared_error: 0.0319 Epoch 00005: val_loss improved from 122.23345 to 83.05646, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 6/100 3501/3501 [==============================] - 0s 95us/step - loss: 103.4990 - mean_squared_error: 0.0420 - val_loss: 90.4268 - val_mean_squared_error: 0.0360 Epoch 00006: val_loss did not improve from 83.05646 Epoch 7/100 3501/3501 [==============================] - 0s 91us/step - loss: 94.0652 - mean_squared_error: 0.0377 - val_loss: 69.3914 - val_mean_squared_error: 0.0264 Epoch 00007: val_loss improved from 83.05646 to 69.39142, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 8/100 3501/3501 [==============================] - 0s 90us/step - loss: 90.3890 - mean_squared_error: 0.0359 - val_loss: 76.0256 - val_mean_squared_error: 0.0288 Epoch 00008: val_loss did not improve from 69.39142 Epoch 9/100 3501/3501 [==============================] - 0s 95us/step - loss: 77.4509 - mean_squared_error: 0.0297 - val_loss: 96.6011 - val_mean_squared_error: 0.0389 Epoch 00009: val_loss did not improve from 69.39142 Epoch 10/100 3501/3501 [==============================] - 0s 93us/step - loss: 77.4043 - mean_squared_error: 0.0296 - val_loss: 63.8580 - val_mean_squared_error: 0.0227 Epoch 00010: val_loss improved from 69.39142 to 63.85800, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 11/100 3501/3501 [==============================] - 0s 87us/step - loss: 70.3015 - mean_squared_error: 0.0260 - val_loss: 55.9432 - val_mean_squared_error: 0.0197 Epoch 00011: val_loss improved from 63.85800 to 55.94323, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 12/100 3501/3501 [==============================] - 0s 84us/step - loss: 71.4479 - mean_squared_error: 0.0262 - val_loss: 61.9059 - val_mean_squared_error: 0.0214 Epoch 00012: val_loss did not improve from 55.94323 Epoch 13/100 3501/3501 [==============================] - 0s 84us/step - loss: 80.7295 - mean_squared_error: 0.0305 - val_loss: 77.4515 - val_mean_squared_error: 0.0290 Epoch 00013: val_loss did not improve from 55.94323 Epoch 14/100 3501/3501 [==============================] - 0s 86us/step - loss: 78.9191 - mean_squared_error: 0.0291 - val_loss: 59.7105 - val_mean_squared_error: 0.0202 Epoch 00014: val_loss did not improve from 55.94323 Epoch 15/100 3501/3501 [==============================] - 0s 82us/step - loss: 70.0246 - mean_squared_error: 0.0251 - val_loss: 63.7124 - val_mean_squared_error: 0.0223 Epoch 00015: val_loss did not improve from 55.94323 Epoch 16/100 3501/3501 [==============================] - 0s 85us/step - loss: 62.7666 - mean_squared_error: 0.0216 - val_loss: 76.1868 - val_mean_squared_error: 0.0290 Epoch 00016: val_loss did not improve from 55.94323 Epoch 17/100 3501/3501 [==============================] - 0s 85us/step - loss: 63.8457 - mean_squared_error: 0.0221 - val_loss: 60.5681 - val_mean_squared_error: 0.0233 Epoch 00017: val_loss did not improve from 55.94323 Epoch 18/100 3501/3501 [==============================] - 0s 81us/step - loss: 62.1890 - mean_squared_error: 0.0214 - val_loss: 59.6436 - val_mean_squared_error: 0.0207 Epoch 00018: val_loss did not improve from 55.94323 Epoch 19/100 3501/3501 [==============================] - 0s 86us/step - loss: 56.6178 - mean_squared_error: 0.0189 - val_loss: 48.2721 - val_mean_squared_error: 0.0182 Epoch 00019: val_loss improved from 55.94323 to 48.27212, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 20/100 3501/3501 [==============================] - 0s 82us/step - loss: 62.5801 - mean_squared_error: 0.0214 - val_loss: 110.6859 - val_mean_squared_error: 0.0466 Epoch 00020: val_loss did not improve from 48.27212 Epoch 21/100 3501/3501 [==============================] - 0s 91us/step - loss: 73.1901 - mean_squared_error: 0.0259 - val_loss: 96.0680 - val_mean_squared_error: 0.0349 Epoch 00021: val_loss did not improve from 48.27212 Epoch 22/100 3501/3501 [==============================] - 0s 82us/step - loss: 77.1672 - mean_squared_error: 0.0264 - val_loss: 88.1257 - val_mean_squared_error: 0.0343 Epoch 00022: val_loss did not improve from 48.27212 Epoch 23/100 3501/3501 [==============================] - 0s 93us/step - loss: 95.5942 - mean_squared_error: 0.0357 - val_loss: 88.5442 - val_mean_squared_error: 0.0318 Epoch 00023: val_loss did not improve from 48.27212 Epoch 24/100 3501/3501 [==============================] - 0s 85us/step - loss: 77.6427 - mean_squared_error: 0.0288 - val_loss: 75.1405 - val_mean_squared_error: 0.0284 Epoch 00024: val_loss did not improve from 48.27212 Epoch 25/100 3501/3501 [==============================] - 0s 85us/step - loss: 74.1847 - mean_squared_error: 0.0278 - val_loss: 61.6067 - val_mean_squared_error: 0.0214 Epoch 00025: val_loss did not improve from 48.27212 Epoch 26/100 3501/3501 [==============================] - 0s 98us/step - loss: 70.3399 - mean_squared_error: 0.0255 - val_loss: 71.1625 - val_mean_squared_error: 0.0267 Epoch 00026: val_loss did not improve from 48.27212 Epoch 27/100 3501/3501 [==============================] - 0s 97us/step - loss: 58.3960 - mean_squared_error: 0.0190 - val_loss: 63.5285 - val_mean_squared_error: 0.0209 Epoch 00027: val_loss did not improve from 48.27212 Epoch 28/100 3501/3501 [==============================] - 0s 84us/step - loss: 69.1214 - mean_squared_error: 0.0247 - val_loss: 64.5801 - val_mean_squared_error: 0.0220 Epoch 00028: val_loss did not improve from 48.27212 Epoch 29/100 3501/3501 [==============================] - 0s 84us/step - loss: 57.8912 - mean_squared_error: 0.0191 - val_loss: 71.4037 - val_mean_squared_error: 0.0234 Epoch 00029: val_loss did not improve from 48.27212 Epoch 30/100 3501/3501 [==============================] - 0s 84us/step - loss: 63.8197 - mean_squared_error: 0.0210 - val_loss: 56.9146 - val_mean_squared_error: 0.0180 Epoch 00030: val_loss did not improve from 48.27212 Epoch 31/100 3501/3501 [==============================] - 0s 86us/step - loss: 56.7003 - mean_squared_error: 0.0185 - val_loss: 44.6653 - val_mean_squared_error: 0.0125 Epoch 00031: val_loss improved from 48.27212 to 44.66530, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 32/100 3501/3501 [==============================] - 0s 84us/step - loss: 68.5283 - mean_squared_error: 0.0245 - val_loss: 65.1528 - val_mean_squared_error: 0.0232 Epoch 00032: val_loss did not improve from 44.66530 Epoch 33/100 3501/3501 [==============================] - 0s 80us/step - loss: 66.3796 - mean_squared_error: 0.0239 - val_loss: 41.1790 - val_mean_squared_error: 0.0116 Epoch 00033: val_loss improved from 44.66530 to 41.17902, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 34/100 3501/3501 [==============================] - 0s 88us/step - loss: 56.4764 - mean_squared_error: 0.0191 - val_loss: 46.3807 - val_mean_squared_error: 0.0152 Epoch 00034: val_loss did not improve from 41.17902 Epoch 35/100 3501/3501 [==============================] - 0s 89us/step - loss: 53.3491 - mean_squared_error: 0.0176 - val_loss: 68.8576 - val_mean_squared_error: 0.0241 Epoch 00035: val_loss did not improve from 41.17902 Epoch 36/100 3501/3501 [==============================] - 0s 82us/step - loss: 59.1346 - mean_squared_error: 0.0199 - val_loss: 42.5398 - val_mean_squared_error: 0.0122 Epoch 00036: val_loss did not improve from 41.17902 Epoch 37/100 3501/3501 [==============================] - 0s 94us/step - loss: 59.4343 - mean_squared_error: 0.0201 - val_loss: 106.8294 - val_mean_squared_error: 0.0415 Epoch 00037: val_loss did not improve from 41.17902 Epoch 38/100 3501/3501 [==============================] - 0s 91us/step - loss: 67.6754 - mean_squared_error: 0.0214 - val_loss: 81.3590 - val_mean_squared_error: 0.0301 Epoch 00038: val_loss did not improve from 41.17902 Epoch 39/100 3501/3501 [==============================] - 0s 88us/step - loss: 71.4328 - mean_squared_error: 0.0257 - val_loss: 65.9904 - val_mean_squared_error: 0.0234 Epoch 00039: val_loss did not improve from 41.17902 Epoch 40/100 3501/3501 [==============================] - 0s 82us/step - loss: 53.6078 - mean_squared_error: 0.0167 - val_loss: 56.5115 - val_mean_squared_error: 0.0181 Epoch 00040: val_loss did not improve from 41.17902 Epoch 41/100 3501/3501 [==============================] - 0s 92us/step - loss: 52.1668 - mean_squared_error: 0.0159 - val_loss: 61.3523 - val_mean_squared_error: 0.0209 Epoch 00041: val_loss did not improve from 41.17902 Epoch 42/100 3501/3501 [==============================] - 0s 88us/step - loss: 67.1410 - mean_squared_error: 0.0232 - val_loss: 53.4885 - val_mean_squared_error: 0.0151 Epoch 00042: val_loss did not improve from 41.17902 Epoch 43/100 3501/3501 [==============================] - 0s 83us/step - loss: 60.4681 - mean_squared_error: 0.0200 - val_loss: 55.0059 - val_mean_squared_error: 0.0159 Epoch 00043: val_loss did not improve from 41.17902 Epoch 44/100 3501/3501 [==============================] - 0s 87us/step - loss: 57.7472 - mean_squared_error: 0.0193 - val_loss: 45.8398 - val_mean_squared_error: 0.0138 Epoch 00044: val_loss did not improve from 41.17902 Epoch 45/100 3501/3501 [==============================] - 0s 91us/step - loss: 53.6203 - mean_squared_error: 0.0175 - val_loss: 98.3426 - val_mean_squared_error: 0.0347 Epoch 00045: val_loss did not improve from 41.17902 Epoch 46/100 3501/3501 [==============================] - 0s 89us/step - loss: 60.2016 - mean_squared_error: 0.0195 - val_loss: 43.7904 - val_mean_squared_error: 0.0122 Epoch 00046: val_loss did not improve from 41.17902 Epoch 47/100 3501/3501 [==============================] - 0s 89us/step - loss: 45.7843 - mean_squared_error: 0.0137 - val_loss: 65.1359 - val_mean_squared_error: 0.0231 Epoch 00047: val_loss did not improve from 41.17902 Epoch 48/100 3501/3501 [==============================] - 0s 95us/step - loss: 52.4230 - mean_squared_error: 0.0160 - val_loss: 52.3943 - val_mean_squared_error: 0.0153 Epoch 00048: val_loss did not improve from 41.17902 Epoch 49/100 3501/3501 [==============================] - 0s 90us/step - loss: 60.3948 - mean_squared_error: 0.0205 - val_loss: 67.5678 - val_mean_squared_error: 0.0249 Epoch 00049: val_loss did not improve from 41.17902 Epoch 50/100 3501/3501 [==============================] - 0s 87us/step - loss: 54.3028 - mean_squared_error: 0.0173 - val_loss: 34.3592 - val_mean_squared_error: 0.0087 Epoch 00050: val_loss improved from 41.17902 to 34.35917, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 51/100 3501/3501 [==============================] - 0s 92us/step - loss: 50.0578 - mean_squared_error: 0.0156 - val_loss: 48.7010 - val_mean_squared_error: 0.0151 Epoch 00051: val_loss did not improve from 34.35917 Epoch 52/100 3501/3501 [==============================] - 0s 96us/step - loss: 55.7035 - mean_squared_error: 0.0182 - val_loss: 65.1687 - val_mean_squared_error: 0.0238 Epoch 00052: val_loss did not improve from 34.35917 Epoch 53/100 3501/3501 [==============================] - 0s 88us/step - loss: 47.7227 - mean_squared_error: 0.0145 - val_loss: 48.0725 - val_mean_squared_error: 0.0130 Epoch 00053: val_loss did not improve from 34.35917 Epoch 54/100 3501/3501 [==============================] - 0s 94us/step - loss: 45.5192 - mean_squared_error: 0.0134 - val_loss: 67.9560 - val_mean_squared_error: 0.0258 Epoch 00054: val_loss did not improve from 34.35917 Epoch 55/100 3501/3501 [==============================] - 0s 89us/step - loss: 52.0394 - mean_squared_error: 0.0165 - val_loss: 56.2932 - val_mean_squared_error: 0.0166 Epoch 00055: val_loss did not improve from 34.35917 Epoch 56/100 3501/3501 [==============================] - 0s 96us/step - loss: 56.6267 - mean_squared_error: 0.0185 - val_loss: 54.9323 - val_mean_squared_error: 0.0178 Epoch 00056: val_loss did not improve from 34.35917 Epoch 57/100 3501/3501 [==============================] - 0s 92us/step - loss: 50.9460 - mean_squared_error: 0.0164 - val_loss: 38.6571 - val_mean_squared_error: 0.0108 Epoch 00057: val_loss did not improve from 34.35917 Epoch 58/100 3501/3501 [==============================] - 0s 89us/step - loss: 51.7611 - mean_squared_error: 0.0169 - val_loss: 55.8965 - val_mean_squared_error: 0.0190 Epoch 00058: val_loss did not improve from 34.35917 Epoch 59/100 3501/3501 [==============================] - 0s 99us/step - loss: 46.6769 - mean_squared_error: 0.0136 - val_loss: 52.6628 - val_mean_squared_error: 0.0161 Epoch 00059: val_loss did not improve from 34.35917 Epoch 60/100 3501/3501 [==============================] - 0s 92us/step - loss: 49.5038 - mean_squared_error: 0.0157 - val_loss: 49.0351 - val_mean_squared_error: 0.0148 Epoch 00060: val_loss did not improve from 34.35917 Epoch 61/100 3501/3501 [==============================] - 0s 100us/step - loss: 44.5845 - mean_squared_error: 0.0130 - val_loss: 41.3265 - val_mean_squared_error: 0.0111 Epoch 00061: val_loss did not improve from 34.35917 Epoch 62/100 3501/3501 [==============================] - 0s 89us/step - loss: 44.9681 - mean_squared_error: 0.0133 - val_loss: 50.5358 - val_mean_squared_error: 0.0150 Epoch 00062: val_loss did not improve from 34.35917 Epoch 63/100 3501/3501 [==============================] - 0s 96us/step - loss: 45.5372 - mean_squared_error: 0.0137 - val_loss: 73.5032 - val_mean_squared_error: 0.0270 Epoch 00063: val_loss did not improve from 34.35917 Epoch 64/100 3501/3501 [==============================] - 0s 95us/step - loss: 54.0177 - mean_squared_error: 0.0179 - val_loss: 106.0356 - val_mean_squared_error: 0.0440 Epoch 00064: val_loss did not improve from 34.35917 Epoch 65/100 3501/3501 [==============================] - 0s 89us/step - loss: 47.1428 - mean_squared_error: 0.0146 - val_loss: 95.2780 - val_mean_squared_error: 0.0395 Epoch 00065: val_loss did not improve from 34.35917 Epoch 66/100 3501/3501 [==============================] - 0s 90us/step - loss: 59.2134 - mean_squared_error: 0.0204 - val_loss: 68.6873 - val_mean_squared_error: 0.0244 Epoch 00066: val_loss did not improve from 34.35917 Epoch 67/100 3501/3501 [==============================] - 0s 96us/step - loss: 48.6599 - mean_squared_error: 0.0149 - val_loss: 50.7604 - val_mean_squared_error: 0.0158 Epoch 00067: val_loss did not improve from 34.35917 Epoch 68/100 3501/3501 [==============================] - 0s 95us/step - loss: 49.0834 - mean_squared_error: 0.0157 - val_loss: 94.0569 - val_mean_squared_error: 0.0375 Epoch 00068: val_loss did not improve from 34.35917 Epoch 69/100 3501/3501 [==============================] - 0s 94us/step - loss: 44.7533 - mean_squared_error: 0.0137 - val_loss: 55.5269 - val_mean_squared_error: 0.0185 Epoch 00069: val_loss did not improve from 34.35917 Epoch 70/100 3501/3501 [==============================] - 0s 90us/step - loss: 45.2486 - mean_squared_error: 0.0135 - val_loss: 39.4384 - val_mean_squared_error: 0.0112 Epoch 00070: val_loss did not improve from 34.35917 Epoch 71/100 3501/3501 [==============================] - 0s 93us/step - loss: 43.2798 - mean_squared_error: 0.0127 - val_loss: 50.4140 - val_mean_squared_error: 0.0162 Epoch 00071: val_loss did not improve from 34.35917 Epoch 72/100 3501/3501 [==============================] - 0s 85us/step - loss: 40.9322 - mean_squared_error: 0.0114 - val_loss: 45.7334 - val_mean_squared_error: 0.0137 Epoch 00072: val_loss did not improve from 34.35917 Epoch 73/100 3501/3501 [==============================] - 0s 86us/step - loss: 42.1467 - mean_squared_error: 0.0123 - val_loss: 57.9899 - val_mean_squared_error: 0.0226 Epoch 00073: val_loss did not improve from 34.35917 Epoch 74/100 3501/3501 [==============================] - 0s 94us/step - loss: 43.6932 - mean_squared_error: 0.0130 - val_loss: 37.1455 - val_mean_squared_error: 0.0100 Epoch 00074: val_loss did not improve from 34.35917 Epoch 75/100 3501/3501 [==============================] - 0s 90us/step - loss: 41.5662 - mean_squared_error: 0.0118 - val_loss: 36.2863 - val_mean_squared_error: 0.0117 Epoch 00075: val_loss did not improve from 34.35917 Epoch 76/100 3501/3501 [==============================] - 0s 92us/step - loss: 46.4817 - mean_squared_error: 0.0140 - val_loss: 45.2458 - val_mean_squared_error: 0.0143 Epoch 00076: val_loss did not improve from 34.35917 Epoch 77/100 3501/3501 [==============================] - 0s 93us/step - loss: 44.4898 - mean_squared_error: 0.0133 - val_loss: 33.5857 - val_mean_squared_error: 0.0093 Epoch 00077: val_loss improved from 34.35917 to 33.58571, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 78/100 3501/3501 [==============================] - 0s 103us/step - loss: 48.6680 - mean_squared_error: 0.0153 - val_loss: 53.6324 - val_mean_squared_error: 0.0177 Epoch 00078: val_loss did not improve from 33.58571 Epoch 79/100 3501/3501 [==============================] - 0s 90us/step - loss: 45.3376 - mean_squared_error: 0.0134 - val_loss: 43.4611 - val_mean_squared_error: 0.0130 Epoch 00079: val_loss did not improve from 33.58571 Epoch 80/100 3501/3501 [==============================] - 0s 93us/step - loss: 46.1038 - mean_squared_error: 0.0140 - val_loss: 39.0098 - val_mean_squared_error: 0.0101 Epoch 00080: val_loss did not improve from 33.58571 Epoch 81/100 3501/3501 [==============================] - 0s 93us/step - loss: 52.8147 - mean_squared_error: 0.0169 - val_loss: 36.5853 - val_mean_squared_error: 0.0086 Epoch 00081: val_loss did not improve from 33.58571 Epoch 82/100 3501/3501 [==============================] - 0s 93us/step - loss: 47.0342 - mean_squared_error: 0.0145 - val_loss: 36.4819 - val_mean_squared_error: 0.0085 Epoch 00082: val_loss did not improve from 33.58571 Epoch 83/100 3501/3501 [==============================] - 0s 96us/step - loss: 40.4257 - mean_squared_error: 0.0111 - val_loss: 48.5772 - val_mean_squared_error: 0.0154 Epoch 00083: val_loss did not improve from 33.58571 Epoch 84/100 3501/3501 [==============================] - 0s 93us/step - loss: 42.8431 - mean_squared_error: 0.0122 - val_loss: 27.9591 - val_mean_squared_error: 0.0072 Epoch 00084: val_loss improved from 33.58571 to 27.95906, saving model to aen_sin_003Hz_n=00128_corgi.h5 Epoch 85/100 3501/3501 [==============================] - 0s 93us/step - loss: 35.2204 - mean_squared_error: 0.0086 - val_loss: 35.1120 - val_mean_squared_error: 0.0119 Epoch 00085: val_loss did not improve from 27.95906 Epoch 86/100 3501/3501 [==============================] - 0s 100us/step - loss: 36.7214 - mean_squared_error: 0.0092 - val_loss: 37.8777 - val_mean_squared_error: 0.0104 Epoch 00086: val_loss did not improve from 27.95906 Epoch 87/100 3501/3501 [==============================] - 0s 94us/step - loss: 42.0163 - mean_squared_error: 0.0119 - val_loss: 37.8636 - val_mean_squared_error: 0.0102 Epoch 00087: val_loss did not improve from 27.95906 Epoch 88/100 3501/3501 [==============================] - 0s 86us/step - loss: 36.6429 - mean_squared_error: 0.0097 - val_loss: 34.5471 - val_mean_squared_error: 0.0107 Epoch 00088: val_loss did not improve from 27.95906 Epoch 89/100 3501/3501 [==============================] - 0s 94us/step - loss: 35.6986 - mean_squared_error: 0.0089 - val_loss: 35.7121 - val_mean_squared_error: 0.0091 Epoch 00089: val_loss did not improve from 27.95906 Epoch 90/100 3501/3501 [==============================] - 0s 96us/step - loss: 47.0230 - mean_squared_error: 0.0144 - val_loss: 66.8385 - val_mean_squared_error: 0.0234 Epoch 00090: val_loss did not improve from 27.95906 Epoch 91/100 3501/3501 [==============================] - 0s 91us/step - loss: 47.1317 - mean_squared_error: 0.0142 - val_loss: 43.3211 - val_mean_squared_error: 0.0132 Epoch 00091: val_loss did not improve from 27.95906 Epoch 92/100 3501/3501 [==============================] - 0s 95us/step - loss: 44.9765 - mean_squared_error: 0.0133 - val_loss: 49.7922 - val_mean_squared_error: 0.0167 Epoch 00092: val_loss did not improve from 27.95906 Epoch 93/100 3501/3501 [==============================] - 0s 102us/step - loss: 43.3008 - mean_squared_error: 0.0126 - val_loss: 34.1182 - val_mean_squared_error: 0.0089 Epoch 00093: val_loss did not improve from 27.95906 Epoch 94/100 3501/3501 [==============================] - 0s 94us/step - loss: 40.7637 - mean_squared_error: 0.0113 - val_loss: 52.4609 - val_mean_squared_error: 0.0177 Epoch 00094: val_loss did not improve from 27.95906 Epoch 95/100 3501/3501 [==============================] - 0s 85us/step - loss: 38.3485 - mean_squared_error: 0.0102 - val_loss: 35.5032 - val_mean_squared_error: 0.0097 Epoch 00095: val_loss did not improve from 27.95906 Epoch 96/100 3501/3501 [==============================] - 0s 87us/step - loss: 38.5916 - mean_squared_error: 0.0102 - val_loss: 32.2546 - val_mean_squared_error: 0.0102 Epoch 00096: val_loss did not improve from 27.95906 Epoch 97/100 3501/3501 [==============================] - 0s 92us/step - loss: 46.0180 - mean_squared_error: 0.0136 - val_loss: 33.4016 - val_mean_squared_error: 0.0048 Epoch 00097: val_loss did not improve from 27.95906 Epoch 98/100 3501/3501 [==============================] - 0s 92us/step - loss: 33.3350 - mean_squared_error: 0.0071 - val_loss: 35.8605 - val_mean_squared_error: 0.0088 Epoch 00098: val_loss did not improve from 27.95906 Epoch 99/100 3501/3501 [==============================] - 0s 86us/step - loss: 47.4055 - mean_squared_error: 0.0145 - val_loss: 67.7614 - val_mean_squared_error: 0.0238 Epoch 00099: val_loss did not improve from 27.95906 Epoch 100/100 3501/3501 [==============================] - 0s 91us/step - loss: 62.3434 - mean_squared_error: 0.0192 - val_loss: 41.4646 - val_mean_squared_error: 0.0117 Epoch 00100: val_loss did not improve from 27.95906 In [13]: # QC training and validation curves (should follow eachother) plt . figure ( figsize = ( 8 , 2 )) plt . plot ( results . history [ 'val_loss' ], label = 'val' ) plt . plot ( results . history [ 'loss' ], label = 'train' ) plt . xlabel ( 'epoch index' ) plt . ylabel ( 'loss value (MSE)' ) plt . legend () plt . show () In [14]: encoded_test = np . array ( encoder . predict ( x_test )) vae_test = vae . predict ( x_test ) First, let's check to see how well the encoder is working. In [15]: plt . plot ( x_test [ 0 ], label = 'true signal' ) plt . plot ( vae_test [ 0 ], label = 'encoded-decoded signal' ) plt . legend () plt . show () Now let's look at the distribution of samples in the latent space, to see how gaussian it is. In [16]: import matplotlib.mlab as mlab n , bins , patches = plt . hist ( encoded_test [ 2 ,:,:] . flatten (), bins = 10 , density = True ) plt . plot ( bins , scipy . stats . norm . pdf ( bins , 0 , 1 )) plt . show () Now let's see how the decoded signal depends on the latent vector. In [17]: from matplotlib.pylab import cm plt . figure ( figsize = ( 10 , 4 )) latent_inputs = np . repeat ( np . linspace ( - 3 , 3 , 11 )[:, np . newaxis ], encoding_dim , axis = 1 ) decoded_latent_inputs = decoder . predict ( latent_inputs ) colors = cm . viridis ( np . linspace ( 0 , 1 , len ( latent_inputs ))) for i , latent_input in enumerate ( decoded_latent_inputs ): plt . plot ( t , latent_input , color = colors [ i ]) labels = [ \" {0:.1f} \" . format ( l ) for l in latent_inputs [:, 0 ]] plt . legend ( labels ) plt . show () Interestingly, the autoencoder has learned to use the latent vector (value in this case, since we specified an encoding dimension of 1) as a proxy for phase. As the latent value changes, the phase of the decoded signal changes. Latent values near zero reproduce a sine wave well, while values far from zero produce signals that aren't exactly sinusoidal. In [ ]:","title":"Variational Autoencoders"},{"location":"projects/auto-encoder/vae/#variational-autoencoders","text":"Variational autoencoders view autoencoding from a statistical perspective. Like classical autoencoders, they encode a dataset into a lower dimensional latent space. Additionally, though, variational autoencoders constrain the encoded vectors to roughly follow a probability distribution, e.g. a normal distribution. Here\u2019s an example of a variational autoencoder for the same 1D sequence to sequence monochromatic signal encoding problem.","title":"Variational Autoencoders"},{"location":"projects/bane-data-science/","text":"Bane\u2019s Data Science Project \u00b6 Here is a description of Bane\u2019s Project! updates are coming\u2026 in the meantime, here is a second way of embedding Jupyter notebooks (this notebook has nothing to do with my project):","title":"Bane's Data Science Project"},{"location":"projects/bane-data-science/#banes-data-science-project","text":"Here is a description of Bane\u2019s Project! updates are coming\u2026 in the meantime, here is a second way of embedding Jupyter notebooks (this notebook has nothing to do with my project):","title":"Bane's Data Science Project"},{"location":"projects/signal_separation/denoiser_k_sparse/","text":"Sparse signal denoising \u00b6 Sometimes we know a signal of interest may be sparsely represented in a particular domain, for example band limited signals in the Fourier domain. Observations of these signals probably contain noise that is difficult to remove. We can train a neural network to separate signal from noise by teaching it the character of k-sparse signals during training. Separating seismic signals out of noisy observations \u00b6 Goals: \u00b6 Build a small, simple, prototype for separating out seismic signals from noisy observations. Scale and repeat separation for multiple applications to keep the project relevant to research. Applications: \u00b6 a. Separating drone motion from ground motion. b. Denoising Field Camp data and separating out. c. (Jihuyn's project) d. (Arneb's project) Current conclusions: \u00b6 K-sparse signal denoiser possible with dense NN and 1D CNN. Denoising with SNR < 1 is possible with K-sparse signals. More work is needed to generalize to seismic signals, need data! Next: \u00b6 Modify network to output amplitude masks. Try network input and output mask in SWT domain. Eventually try STFT domain. In [1]: import pywt import numpy as np import matplotlib.pyplot as plt from keras.models import Input , Model , load_model from keras.layers import Conv1D , Conv2DTranspose , MaxPooling1D , Lambda from keras.layers.merge import concatenate from keras.callbacks import EarlyStopping , ModelCheckpoint , TensorBoard from keras.utils import plot_model import keras.backend as K from sklearn.cluster import KMeans from sklearn.model_selection import train_test_split from sklearn.preprocessing import scale , StandardScaler , MinMaxScaler import petname Using TensorFlow backend. In [2]: def Conv1DTranspose ( input_tensor , filters , kernel_size , strides = 2 , padding = 'same' ): ''' Construct a transpose layer corresponding to Conv1D ''' x = Lambda ( lambda x : K . expand_dims ( x , axis = 2 ))( input_tensor ) x = Conv2DTranspose ( filters = filters , kernel_size = ( kernel_size , 1 ), strides = ( strides , 1 ), padding = padding )( x ) x = Lambda ( lambda x : K . squeeze ( x , axis = 2 ))( x ) return x def get_signal_k ( n , k , j_noise_levels = [ 1 ], w = pywt . Wavelet ( 'db1' ), power = 1 ): ''' Construts random n-lengthed k-sparse signal Inputs: n - number of noise samples k - sparsity level of noise j_noise_levels = list of levels where noise is allowed w - wavelet to use power = power level of noise (sum of squares) ''' f = np . zeros (( n ,)) # ensure consistent list structure with pywavelets level = pywt . dwt_max_level ( data_len = n , filter_len = w . dec_len ) fnB = pywt . wavedec ( f , w , level = level ) fnB , fnB_slices = pywt . coeffs_to_array ( fnB ) fnB *= 0 # populate random indices with random coeff. for j in j_noise_levels : nj = len ( fnB [ fnB_slices [ j ][ 'd' ]]) fnB [ fnB_slices [ j ][ 'd' ] ][ np . random . randint ( 0 , nj , k ) ] = np . random . randn ( k ) + 1 # reconstruct signal fn = pywt . waverecn ( pywt . array_to_coeffs ( fnB , fnB_slices ), w ) # scale power current_power = np . sqrt ( np . linalg . norm ( np . abs ( fn ) ** 2 )) fn = fn / current_power * power return fn def get_signals_k ( n_signals , n , k , j_noise_levels = [ 1 ], w = pywt . Wavelet ( 'db1' ), power = 1 ): ''' Construts n_signal random n-lengthed k-sparse signals Inputs: n_signals - number of signals n - number of noise samples k - sparsity level of noise j_noise_levels = list of levels where noise is allowed w - wavelet to use power = power level of noise (sum of squares) ''' signals = np . zeros (( n , n_signals )) f = np . zeros (( n ,)) # ensure consistent list structure with pywavelets level = pywt . dwt_max_level ( data_len = n , filter_len = w . dec_len ) fnB = pywt . wavedec ( f , w , level = level ) fnB , fnB_slices = pywt . coeffs_to_array ( fnB ) fnB *= 0 for i in range ( n_signals ): # populate random indices with random coeff. for j in j_noise_levels : nj = len ( fnB [ fnB_slices [ j ][ 'd' ]]) fnB [ fnB_slices [ j ][ 'd' ] ][ np . random . randint ( 0 , nj , k ) ] = np . random . randn ( k ) + 1 # reconstruct signal fn = pywt . waverecn ( pywt . array_to_coeffs ( fnB , fnB_slices ), w ) # scale power current_power = np . sqrt ( np . linalg . norm ( np . abs ( fn ) ** 2 )) fn = fn / current_power * power # clear out coeff. structure for next signal fnB *= 0 # store this signal signals [:, i ] = fn . copy () return signals In [151]: # generate signals n_signals = 2 ** 15 n_noise = 2 ** 15 nt = 128 k = 5 f = get_signals_k ( n_signals = n_signals , n = nt , k = k , j_noise_levels = [ 2 ], w = pywt . Wavelet ( 'db8' ), power = 1 ) f = f . T In [152]: # generate noise h = np . random . randn ( n_noise , nt ) / 4 In [153]: # QC a few signals plt . figure ( figsize = ( 10 , 3 )) plt . plot ( f [ 1 ,:] + h [ 1 ,:], label = 'noisy signal' ) plt . plot ( f [ 1 ,:], label = 'signal' ) plt . legend () plt . show () In [154]: # generate noisy observations x = f + h In [155]: # split noisy observations and corresponding labels into test, validation, and training sets x_temp , x_test , y_temp , y_test = train_test_split ( x , f , test_size = 0.05 ) x_train , x_valid , y_train , y_valid = train_test_split ( x_temp , y_temp , test_size = 0.1 ) n_train = len ( x_train ) n_valid = len ( x_valid ) n_test = len ( x_test ) In [156]: # construct 1D convolutional neural unet nchannels = 1 activation = 'tanh' inputs = Input (( nt , nchannels )) c1 = Conv1D ( 8 , 3 , activation = activation , padding = 'same' )( inputs ) c1 = Conv1D ( 8 , 3 , activation = activation , padding = 'same' )( c1 ) p1 = MaxPooling1D ( 2 )( c1 ) c2 = Conv1D ( 16 , 3 , activation = activation , padding = 'same' ) ( p1 ) c2 = Conv1D ( 16 , 3 , activation = activation , padding = 'same' ) ( c2 ) p2 = MaxPooling1D ( pool_size = 2 ) ( c2 ) c3 = Conv1D ( 32 , 3 , activation = activation , padding = 'same' ) ( p2 ) c3 = Conv1D ( 32 , 3 , activation = activation , padding = 'same' ) ( c3 ) p3 = MaxPooling1D ( pool_size = 2 ) ( c3 ) c4 = Conv1D ( 64 , 3 , activation = activation , padding = 'same' ) ( p3 ) c4 = Conv1D ( 64 , 3 , activation = activation , padding = 'same' ) ( c4 ) p4 = MaxPooling1D ( pool_size = 2 ) ( c4 ) c5 = Conv1D ( 128 , 3 , activation = activation , padding = 'same' ) ( p4 ) c5 = Conv1D ( 128 , 3 , activation = activation , padding = 'same' ) ( c5 ) u6 = Conv1DTranspose ( c5 , 64 , 2 , strides = 2 , padding = 'same' ) u6 = concatenate ([ u6 , c4 ]) c6 = Conv1D ( 64 , 3 , activation = activation , padding = 'same' ) ( u6 ) c6 = Conv1D ( 64 , 3 , activation = activation , padding = 'same' ) ( c6 ) u7 = Conv1DTranspose ( c6 , 32 , 2 , strides = 2 , padding = 'same' ) u7 = concatenate ([ u7 , c3 ]) c7 = Conv1D ( 32 , 3 , activation = activation , padding = 'same' ) ( u7 ) c7 = Conv1D ( 32 , 3 , activation = activation , padding = 'same' ) ( c7 ) u8 = Conv1DTranspose ( c7 , 16 , 2 , strides = 2 , padding = 'same' ) u8 = concatenate ([ u8 , c2 ]) c8 = Conv1D ( 16 , 3 , activation = activation , padding = 'same' ) ( u8 ) c8 = Conv1D ( 16 , 3 , activation = activation , padding = 'same' ) ( c8 ) u9 = Conv1DTranspose ( c8 , 8 , 2 , strides = 2 , padding = 'same' ) u9 = concatenate ([ u9 , c1 ], axis = 2 ) c9 = Conv1D ( 8 , 3 , activation = activation , padding = 'same' ) ( u9 ) c9 = Conv1D ( 8 , 3 , activation = activation , padding = 'same' ) ( c9 ) outputs = Conv1D ( 1 , 1 , activation = 'tanh' ) ( c9 ) unet = Model ( inputs = [ inputs ], outputs = [ outputs ]) In [157]: # specify opt. strategy unet . compile ( optimizer = 'adam' , loss = 'mae' , metrics = [ 'mae' ]) # specify training parameters and callback functions batch_size = 256 epochs = 10 unique_name = petname . name () model_filename = 'unet_n= %05d _' % ( nt ) + unique_name + '.h5' history_filename = 'results_' + unique_name + '.npz' earlystopper = EarlyStopping ( patience = 10 , verbose = 1 ) checkpointer = ModelCheckpoint ( model_filename , verbose = 1 , save_best_only = True ) tensorboard = TensorBoard ( log_dir = 'tensorboard_logs/' ) callbacks = [ earlystopper , checkpointer , tensorboard ] In [158]: # train signal separator results = unet . fit ( x_train [:,:, np . newaxis ], y_train [:,:, np . newaxis ], batch_size = batch_size , epochs = epochs , validation_data = ( x_valid [:,:, np . newaxis ], y_valid [:,:, np . newaxis ]), callbacks = callbacks ) Train on 28016 samples, validate on 3113 samples Epoch 1/10 28016/28016 [==============================] - 11s 407us/step - loss: 0.0596 - mean_absolute_error: 0.0596 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382 Epoch 00001: val_loss improved from inf to 0.03818, saving model to unet_n=00128_drake.h5 Epoch 2/10 28016/28016 [==============================] - 10s 373us/step - loss: 0.0350 - mean_absolute_error: 0.0350 - val_loss: 0.0347 - val_mean_absolute_error: 0.0347 Epoch 00002: val_loss improved from 0.03818 to 0.03474, saving model to unet_n=00128_drake.h5 Epoch 3/10 28016/28016 [==============================] - 11s 377us/step - loss: 0.0326 - mean_absolute_error: 0.0326 - val_loss: 0.0311 - val_mean_absolute_error: 0.0311 Epoch 00003: val_loss improved from 0.03474 to 0.03112, saving model to unet_n=00128_drake.h5 Epoch 4/10 28016/28016 [==============================] - 11s 385us/step - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0290 - val_mean_absolute_error: 0.0290 Epoch 00004: val_loss improved from 0.03112 to 0.02904, saving model to unet_n=00128_drake.h5 Epoch 5/10 28016/28016 [==============================] - 11s 389us/step - loss: 0.0287 - mean_absolute_error: 0.0287 - val_loss: 0.0289 - val_mean_absolute_error: 0.0289 Epoch 00005: val_loss improved from 0.02904 to 0.02888, saving model to unet_n=00128_drake.h5 Epoch 6/10 28016/28016 [==============================] - 11s 392us/step - loss: 0.0284 - mean_absolute_error: 0.0284 - val_loss: 0.0279 - val_mean_absolute_error: 0.0279 Epoch 00006: val_loss improved from 0.02888 to 0.02785, saving model to unet_n=00128_drake.h5 Epoch 7/10 28016/28016 [==============================] - 11s 387us/step - loss: 0.0279 - mean_absolute_error: 0.0279 - val_loss: 0.0280 - val_mean_absolute_error: 0.0280 Epoch 00007: val_loss did not improve from 0.02785 Epoch 8/10 28016/28016 [==============================] - 11s 380us/step - loss: 0.0274 - mean_absolute_error: 0.0274 - val_loss: 0.0279 - val_mean_absolute_error: 0.0279 Epoch 00008: val_loss did not improve from 0.02785 Epoch 9/10 28016/28016 [==============================] - 11s 378us/step - loss: 0.0273 - mean_absolute_error: 0.0273 - val_loss: 0.0270 - val_mean_absolute_error: 0.0270 Epoch 00009: val_loss improved from 0.02785 to 0.02698, saving model to unet_n=00128_drake.h5 Epoch 10/10 28016/28016 [==============================] - 11s 378us/step - loss: 0.0270 - mean_absolute_error: 0.0270 - val_loss: 0.0270 - val_mean_absolute_error: 0.0270 Epoch 00010: val_loss did not improve from 0.02698 In [208]: # QC training and validation curves (should follow eachother) plt . figure ( figsize = ( 8 , 3 )) plt . plot ( results . history [ 'val_loss' ], label = 'val' ) plt . plot ( results . history [ 'loss' ], label = 'train' ) plt . xlabel ( 'epoch index' ) plt . ylabel ( 'loss value (MSE)' ) plt . legend () plt . show () In [161]: # denoise testing data denoised = unet . predict ( x_test [:,:, np . newaxis ]) In [ ]: In [209]: # QC that autoencoder can autoencode a sine wave from the test set i = np . random . randint ( 0 , n_test - 1 ) plt . figure ( figsize = ( 8 , 3 )) plt . plot ( denoised [ i ,:], label = 'denoised' ) plt . plot ( x_test [ i ,:], label = 'noisy' ) plt . plot ( y_test [ i ,:], label = 'no noise' , c = 'k' , lw = 3 , alpha = 0.3 ) plt . xlabel ( 'sample' ) plt . ylabel ( 'amplitude' ) plt . legend () plt . show () Tinkering with DWT, IDWT, and SWT. \u00b6 I think the redundancy of the SWT would be a good candidate for inputs to the neural network. In [15]: coeffs = pywt . swt ( f [ 1 ,:], 'db2' , level = 7 ) fh = pywt . iswt ( coeffs , 'db2' ) plt . plot ( f [ 1 ,:]) plt . plot ( fh ) shift = 0 for i in range ( len ( coeffs )): j = 0 #for j in range(len(coeffs[i])): shift += 1 plt . plot ( coeffs [ i ][ j ] + shift , label = '[ %2d ][ %2d ]' % ( i , j )) #plt.plot(coeffs[0][0], label='[0][0]') plt . legend () plt . show () cA , cD = pywt . dwt ( f [ 1 ,:], wavelet = pywt . Wavelet ( 'db7' )) fh = pywt . idwt ( cA , cD , wavelet = pywt . Wavelet ( 'db7' )) plt . plot ( f [ 1 ,:]) plt . plot ( fh ) plt . show ()","title":"Sparse signal denoising"},{"location":"projects/signal_separation/denoiser_k_sparse/#sparse-signal-denoising","text":"Sometimes we know a signal of interest may be sparsely represented in a particular domain, for example band limited signals in the Fourier domain. Observations of these signals probably contain noise that is difficult to remove. We can train a neural network to separate signal from noise by teaching it the character of k-sparse signals during training.","title":"Sparse signal denoising"},{"location":"projects/signal_separation/readme/","text":"Separating and denoising digital signals using various neural networks \u00b6 Examples: denoiser_k_sparse.ipynb - a small simple example of training a NN to denoise k-sparse signals.","title":"Separating and denoising digital signals using various neural networks"},{"location":"projects/signal_separation/readme/#separating-and-denoising-digital-signals-using-various-neural-networks","text":"Examples: denoiser_k_sparse.ipynb - a small simple example of training a NN to denoise k-sparse signals.","title":"Separating and denoising digital signals using various neural networks"},{"location":"projects/well_log_generator/","text":"Overview \u00b6 Let\u2019s train a neural network to generate well logs! A generator can be used to interpolate missing data, or as implicit regularization for inversion. Step 1: Find digital well logs for training. An easy way to download thousands of well logs in bulk is to get them from the Kansas Geological Survey . Step 2: Prepare the data to use for training. We\u2019ve gone through and selected just over 4000 wells to use as a training set. These are all the wells that have uninterrupted RILD logs in the depth interval from 400 ft to 4000 ft, sampled every foot. For a more tractable problem, we also downsampled this data by averaging linearly over 60 ft intervals, resulting in logs that have 60 data points each, spaced 60 ft apart. This downsampled training data is saved in a Github repository . Step 3: Choose a generator algorithm. There are a few ways to do this. One is to build a variational autoencoder, or VAE (see the variational autoencoder project for more). Another is to build a generative adversarial network (GAN). Thirdly, we could combine the two to form a VAE-GAN; see here for an explanation. Ideally, let\u2019s try all three and compare the results!","title":"Overview"},{"location":"projects/well_log_generator/#overview","text":"Let\u2019s train a neural network to generate well logs! A generator can be used to interpolate missing data, or as implicit regularization for inversion. Step 1: Find digital well logs for training. An easy way to download thousands of well logs in bulk is to get them from the Kansas Geological Survey . Step 2: Prepare the data to use for training. We\u2019ve gone through and selected just over 4000 wells to use as a training set. These are all the wells that have uninterrupted RILD logs in the depth interval from 400 ft to 4000 ft, sampled every foot. For a more tractable problem, we also downsampled this data by averaging linearly over 60 ft intervals, resulting in logs that have 60 data points each, spaced 60 ft apart. This downsampled training data is saved in a Github repository . Step 3: Choose a generator algorithm. There are a few ways to do this. One is to build a variational autoencoder, or VAE (see the variational autoencoder project for more). Another is to build a generative adversarial network (GAN). Thirdly, we could combine the two to form a VAE-GAN; see here for an explanation. Ideally, let\u2019s try all three and compare the results!","title":"Overview"},{"location":"projects/well_log_generator/GAN/","text":"Generative Adversarial Network \u00b6 A Generative Adversarial Network (GAN) consists of two neural networks, a generator and a discriminator, that are trained simultaneously. The generator takes a randomly-drawn latent vector and produces an image (here, a well log). The discriminator takes an image (well log), and classifies it as belonging to the training set, or as coming from the generator. As the generator gets better at producing images, the discriminator must get better at telling generated images from training images. The goal is to train the generator to produce images that are as similar to the training set as possible. Here, we trained a GAN on RILD logs from the Kansas Geological survey. This is a bit different than the usual application of GANs since the training set consists of 1D data, not 2D pictures. Training a generative adversarial network to produce resistivity logs \u00b6 Here we train a GAN on resistivity well logs from the Kansas Geological Survey Imports \u00b6 In [0]: % matplotlib inline import os import numpy as np import pandas as pd import time import matplotlib.pyplot as plt from IPython import display from keras.models import Sequential , Model from keras.layers import Input , Dense , Activation , Flatten , Reshape from keras.layers import Conv1D , Conv2DTranspose , ZeroPadding1D , UpSampling1D from keras.layers import LeakyReLU , Dropout from keras.layers import BatchNormalization , Lambda from keras.optimizers import Adam , RMSprop , SGD from keras import backend as K Using TensorFlow backend. Create a folder to store results in In [0]: run = 't1' if not os . path . exists ( run ): os . makedirs ( run ) Data is stored in a github repo for easy access here In [0]: # load data if not os . path . exists ( 'KGS_RILD_60ft' ): ! git clone https://github.com/MLGeophysics/KGS_RILD_60ft.git x_train = np . load ( 'KGS_RILD_60ft/KSG_RILD_60ft.npy' ) In [0]: # Add channel axis x_train = np . expand_dims ( x_train , axis = 2 ) # convert to log resistivity x_train = np . log10 ( x_train ) # normalize data between -1 and 1 min_data = min ( x_train . flatten ()) max_data = max ( x_train . flatten ()) data_diff = max_data - min_data # pad by norm_pad, so that a bunch of values don't end up at -1 norm_pad = 0.1 norm_diff = data_diff * ( 1. + norm_pad * 2 ) norm_min = min_data - data_diff * norm_pad x_train = 2 * ( x_train - norm_min ) / norm_diff - 1. x_train . shape Out[0]: (4005, 60, 1) Architecture of discriminator and generator \u00b6 Input/output shapes, choice of optimizer In [0]: samples_per_log = x_train . shape [ 1 ] # 60 channels = 1 latent_dim = 100 optimizer = Adam ( 0.0002 , 0.5 ) WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. Define a binary accuracy metric that works for soft labels(not only 0 or 1). In [0]: def soft_binary_accuracy ( y_true , y_pred ): ''' get accuracy even using soft labels ''' return K . mean ( K . equal ( K . round ( y_true ), K . round ( y_pred ))) The discriminator takes in a well log, feeds it through several convolutional layers and a final dense layer, and outputs a value between 0 and 1. In [0]: # Build and compile the discriminator # self.discriminator = self.build_discriminator() D = Sequential () depth = 8 ks = 5 dropout = 0.25 # In: 60 x 1, depth = 1 # Out: 30 x 1, depth=8 input_shape = ( samples_per_log , channels ) D . add ( Conv1D ( depth * 1 , kernel_size = ks , strides = 2 , input_shape = input_shape , padding = 'same' )) D . add ( LeakyReLU ( alpha = 0.2 )) D . add ( Dropout ( rate = dropout )) # In: 30 x 1, depth=8 # Out: 15 x 1, depth=16 D . add ( Conv1D ( depth * 2 , kernel_size = ks , strides = 2 , padding = 'same' )) D . add ( BatchNormalization ( momentum = 0.8 )) D . add ( LeakyReLU ( alpha = 0.2 )) D . add ( Dropout ( rate = dropout )) # In: 15 x 1, depth=16 # Out: 8 x 1, depth=32 D . add ( ZeroPadding1D ( padding = ( 0 , 1 ))) D . add ( Conv1D ( depth * 4 , kernel_size = ks , strides = 2 , padding = 'same' )) D . add ( BatchNormalization ( momentum = 0.8 )) D . add ( LeakyReLU ( alpha = 0.2 )) D . add ( Dropout ( rate = dropout )) # In: 8 x 1, depth=32 # Out: 4 x 1, depth=64 D . add ( Conv1D ( depth * 8 , kernel_size = ks , strides = 2 , padding = 'same' )) D . add ( BatchNormalization ( momentum = 0.8 )) D . add ( LeakyReLU ( alpha = 0.2 )) D . add ( Dropout ( rate = dropout )) # Out: 1-dim probability D . add ( Flatten ()) D . add ( Dense ( 1 , activation = 'sigmoid' )) D . summary () img = Input ( shape = input_shape ) validity = D ( img ) discriminator = Model ( img , validity ) WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv1d_1 (Conv1D) (None, 30, 8) 48 _________________________________________________________________ leaky_re_lu_1 (LeakyReLU) (None, 30, 8) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 30, 8) 0 _________________________________________________________________ conv1d_2 (Conv1D) (None, 15, 16) 656 _________________________________________________________________ batch_normalization_1 (Batch (None, 15, 16) 64 _________________________________________________________________ leaky_re_lu_2 (LeakyReLU) (None, 15, 16) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 15, 16) 0 _________________________________________________________________ zero_padding1d_1 (ZeroPaddin (None, 16, 16) 0 _________________________________________________________________ conv1d_3 (Conv1D) (None, 8, 32) 2592 _________________________________________________________________ batch_normalization_2 (Batch (None, 8, 32) 128 _________________________________________________________________ leaky_re_lu_3 (LeakyReLU) (None, 8, 32) 0 _________________________________________________________________ dropout_3 (Dropout) (None, 8, 32) 0 _________________________________________________________________ conv1d_4 (Conv1D) (None, 4, 64) 10304 _________________________________________________________________ batch_normalization_3 (Batch (None, 4, 64) 256 _________________________________________________________________ leaky_re_lu_4 (LeakyReLU) (None, 4, 64) 0 _________________________________________________________________ dropout_4 (Dropout) (None, 4, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 256) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 257 ================================================================= Total params: 14,305 Trainable params: 14,081 Non-trainable params: 224 _________________________________________________________________ Compile the generator, using the custom-defined metric and the optimizer specified above. In [0]: discriminator . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ soft_binary_accuracy ]) The Generator takes a 100 element vector of numbers drawn randomly from a normal distribution, and outputs a 60-sample RILD well log In [0]: #def build_generator(self): G = Sequential () depth = 32 ks = 5 dropout = 0.25 dim = 15 # In: 100 # Out: 15 x 32 G . add ( Dense ( dim * depth , input_dim = latent_dim )) G . add ( LeakyReLU ( alpha = 0.2 )) # add a dimension to use Conv2DTranspose (Conv1DTranspose doesn't exist) G . add ( Reshape (( dim , 1 , depth ))) # In: 15 x 1 x 32 # Out: 30 x 1 x 16 G . add ( Conv2DTranspose ( filters = depth // 2 , kernel_size = ( ks , 1 ), strides = ( 2 , 1 ), padding = 'same' )) G . add ( BatchNormalization ( momentum = 0.8 )) G . add ( LeakyReLU ( alpha = 0.2 )) # In: 30 x 1 x 16 # Out: 60 x 1 x 8 G . add ( Conv2DTranspose ( filters = depth // 4 , kernel_size = ( ks , 1 ), strides = ( 2 , 1 ), padding = 'same' )) G . add ( BatchNormalization ( momentum = 0.8 )) G . add ( LeakyReLU ( alpha = 0.2 )) # In: 60 x 1 x 8 # Out: 60 x 1 property image G . add ( Reshape (( samples_per_log , - 1 ))) G . add ( Conv1D ( channels , ks , strides = 1 , padding = 'same' )) G . add ( Activation ( 'tanh' )) G . summary () noise = Input ( shape = ( latent_dim ,)) img = G ( noise ) generator = Model ( noise , img ) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 480) 48480 _________________________________________________________________ leaky_re_lu_5 (LeakyReLU) (None, 480) 0 _________________________________________________________________ reshape_1 (Reshape) (None, 15, 1, 32) 0 _________________________________________________________________ conv2d_transpose_1 (Conv2DTr (None, 30, 1, 16) 2576 _________________________________________________________________ batch_normalization_4 (Batch (None, 30, 1, 16) 64 _________________________________________________________________ leaky_re_lu_6 (LeakyReLU) (None, 30, 1, 16) 0 _________________________________________________________________ conv2d_transpose_2 (Conv2DTr (None, 60, 1, 8) 648 _________________________________________________________________ batch_normalization_5 (Batch (None, 60, 1, 8) 32 _________________________________________________________________ leaky_re_lu_7 (LeakyReLU) (None, 60, 1, 8) 0 _________________________________________________________________ reshape_2 (Reshape) (None, 60, 8) 0 _________________________________________________________________ conv1d_5 (Conv1D) (None, 60, 1) 41 _________________________________________________________________ activation_1 (Activation) (None, 60, 1) 0 ================================================================= Total params: 51,841 Trainable params: 51,793 Non-trainable params: 48 _________________________________________________________________ Build the generator, but do not compile. We compile the combined generator-discriminator In [0]: # The generator takes noise as input and generates imgs z = Input ( shape = ( latent_dim ,)) img = generator ( z ) # For the combined model we will only train the generator discriminator . trainable = False # The discriminator takes generated images as input and determines validity valid = discriminator ( img ) # The combined model (stacked generator and discriminator) # trains the generator to fool the discriminator combined = Model ( z , valid ) combined . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ soft_binary_accuracy ]) discriminator . trainable = True In [0]: def plot_16_images ( images , save2file = False , folder = '.' , epoch = 0 , xlim = [ - 1 , 1 ]): ''' plot 16 logs in a 4x4 grid if epoch < 0, assume they are training logs otherwise, assume they're generated after training epoch # epoch ''' fake = epoch >= 0 if fake : filename = folder + \"/RILD_ %05d .png\" % epoch else : filename = folder + '/RILD_train_examples.png' plt . figure ( figsize = ( 10 , 10 )) for i in range ( images . shape [ 0 ]): plt . subplot ( 4 , 4 , i + 1 ) image = images [ i ] plt . plot ( image , np . arange ( len ( image ))) plt . xlim ( xlim ) plt . gca () . invert_yaxis () plt . tight_layout ( rect = [ 0 , 0.03 , 1 , 0.95 ]) if fake : plt . suptitle ( 'Epoch %d ' % epoch ) if save2file : plt . savefig ( filename ) plt . close ( 'all' ) else : display . display ( plt . gcf ()) display . clear_output ( wait = True ) plt . close ( 'all' ) #plt.show() In [0]: # plot training data i_plt_logs = np . random . randint ( 0 , x_train . shape [ 0 ], 16 ) plot_16_images ( x_train [ i_plt_logs ], save2file = True , folder = run , epoch =- 1 ) plot_16_images ( x_train [ i_plt_logs ], save2file = False , folder = run , epoch =- 1 ) Define training hyperparameters \u00b6 In [0]: # Specify training hyperparameters epochs = 20000 batch_size = 32 save_interval = 50 # max value for soft labels softness = 0.1 # number of swapped labels for noisy discriminator labels noisy_labels = False noise_level = 0.05 num_noise = int ( batch_size * noise_level ) # size of experience replay vector exp_size = 100 # rate of replacement of vector # mean age of image ~= 1/exp_update_rate # max age of image ~= 250 for exp_size=100, exp_update_rate = .02 exp_update_rate = 0.02 num_update_exp = int ( exp_size * exp_update_rate ) exp_train = 0.2 num_exp = int ( batch_size * exp_train ) In [0]: # Adversarial ground truths fake = np . ones (( batch_size , 1 )) valid = np . zeros (( batch_size , 1 )) # Save random noise for plots noise_input = np . random . normal ( 0 , 1 , size = [ 16 , latent_dim ]) plt_images = generator . predict ( noise_input ) plot_16_images ( plt_images , save2file = True , folder = run , epoch = 0 ) # Save metadata and metrics metrics = np . empty (( epochs , 5 )) logfile = run + '/log.txt' with open ( logfile , 'w' ) as lf : discriminator . summary ( print_fn = lambda x : lf . write ( x + ' \\n ' )) D . summary ( print_fn = lambda x : lf . write ( x + ' \\n ' )) generator . summary ( print_fn = lambda x : lf . write ( x + ' \\n ' )) G . summary ( print_fn = lambda x : lf . write ( x + ' \\n ' )) # Initialize experience replay vector batch_noise = np . random . normal ( 0 , 1 , ( exp_size , latent_dim )) exp_images = generator . predict ( batch_noise ) Train \u00b6 In [0]: for epoch in range ( epochs ): # --------------------- # Train Discriminator # --------------------- # Select a random batch of images idx = np . random . randint ( 0 , x_train . shape [ 0 ], batch_size ) imgs = x_train [ idx ] # Sample noise and generate a batch of new images batch_noise = np . random . normal ( 0 , 1 , ( batch_size , latent_dim )) gen_imgs = generator . predict ( batch_noise ) # Load a few old generated images i_choose_exp = np . random . randint ( 0 , exp_size , num_exp ) i_swap_exp = np . random . randint ( 0 , batch_size , num_exp ) gen_imgs [ i_swap_exp ] = exp_images [ i_choose_exp ] # Soft labels for discriminator discriminator_fake = np . random . uniform ( 1. - softness , 1. ,( batch_size , 1 )) discriminator_valid = np . random . uniform ( 0. , softness ,( batch_size , 1 )) if noisy_labels : # Noisy labels for discriminator i_swap = np . random . randint ( 0 , batch_size , num_noise ) discriminator_fake [ i_swap ] = np . random . uniform ( 0. , softness ,( num_noise , 1 )) discriminator_valid [ i_swap ] = np . random . uniform ( 1. - softness , 1. ,( num_noise , 1 )) # Train the discriminator (real classified as ones and generated as zeros) d_loss_real = discriminator . train_on_batch ( imgs , discriminator_valid ) d_loss_fake = discriminator . train_on_batch ( gen_imgs , discriminator_fake ) d_loss = 0.5 * np . add ( d_loss_real , d_loss_fake ) # --------------------- # Train Generator # --------------------- # Train the generator (wants discriminator to mistake images as real) g_loss = combined . train_on_batch ( batch_noise , valid ) # Plot and save the progress metric = ( epoch , d_loss [ 0 ], 100 * d_loss [ 1 ], g_loss [ 0 ], 100 * g_loss [ 1 ]) # print_line = \"%d [D loss: %f, acc.: %.2f%%] [G loss: %f, acc.: %.2f%%]\" % metric # print (print_line) # with open(logfile,'a') as lf: # lf.write(print_line+'\\n') metrics [ epoch ] = metric # If at save interval => save generated image samples if ( 1 + epoch ) % save_interval == 0 : #discriminator.save(run+'/discriminator_%05d.h5'%(1+epoch)) #combined.save(run+'/combined_%05d.h5'%(1+epoch)) #generator.save(run+'/generator_%05d.h5'%(1+epoch)) plt_images = generator . predict ( noise_input ) plot_16_images ( plt_images , save2file = False , folder = run , epoch = 1 + epoch ) #np.save(run+'/metrics.npy',metrics) #np.savetxt(run+'/metrics.txt',metrics) # update experience replay vector i_update_exp = np . random . randint ( 0 , exp_size , num_update_exp ) exp_noise = np . random . normal ( 0 , 1 ,( num_update_exp , latent_dim )) exp_images [ i_update_exp ] = generator . predict ( exp_noise ) # save results and metrics discriminator . save ( run + '/discriminator_final.h5' ) combined . save ( run + '/combined_final.h5' ) generator . save ( run + '/generator_final.h5' ) np . save ( run + '/metrics.npy' , metrics ) np . savetxt ( run + '/metrics.txt' , metrics ) plt_images = generator . predict ( noise_input ) plot_16_images ( plt_images , save2file = True , folder = run , epoch = 1 + epoch ) Analyze results \u00b6 In [19]: # plot loss plt . plot ( metrics [:, 0 ], metrics [:, 1 ]) plt . plot ( metrics [:, 0 ], metrics [:, 3 ]) plt . legend ([ 'D' , 'G' ]) plt . show () # plot accuracy plt . plot ( metrics [:, 0 ], metrics [:, 2 ]) plt . plot ( metrics [:, 0 ], metrics [:, 4 ]) plt . legend ([ 'D' , 'G' ]) plt . show () Compare statistics of generator output and training data In [20]: # Generate as many synthetic logs as there are training logs gen_logs_input = np . random . normal ( 0 , 1 ,( x_train . shape [ 0 ], latent_dim )) gen_logs = generator . predict ( gen_logs_input ) gen_logs . shape Out[20]: (4005, 60, 1) In [21]: # means plt . plot ( np . mean ( x_train [:,:, 0 ], axis = 0 ), np . arange ( 60 )) plt . plot ( np . mean ( gen_logs [:,:, 0 ], axis = 0 ), np . arange ( 60 )) plt . gca () . invert_yaxis () plt . legend ([ 'Train' , 'Generator' ]) plt . title ( 'Mean of samples' ) plt . show () # standard deviations plt . plot ( np . std ( x_train [:,:, 0 ], axis = 0 ), np . arange ( 60 )) plt . plot ( np . std ( gen_logs [:,:, 0 ], axis = 0 ), np . arange ( 60 )) plt . gca () . invert_yaxis () plt . legend ([ 'Train' , 'Generator' ]) plt . title ( 'Standard deviation of samples' ) plt . show () In [0]:","title":"Generative Adversarial Network"},{"location":"projects/well_log_generator/GAN/#generative-adversarial-network","text":"A Generative Adversarial Network (GAN) consists of two neural networks, a generator and a discriminator, that are trained simultaneously. The generator takes a randomly-drawn latent vector and produces an image (here, a well log). The discriminator takes an image (well log), and classifies it as belonging to the training set, or as coming from the generator. As the generator gets better at producing images, the discriminator must get better at telling generated images from training images. The goal is to train the generator to produce images that are as similar to the training set as possible. Here, we trained a GAN on RILD logs from the Kansas Geological survey. This is a bit different than the usual application of GANs since the training set consists of 1D data, not 2D pictures.","title":"Generative Adversarial Network"},{"location":"resources/list/","text":"List of Resources \u00b6 TensorFlow Tutorials \u00b6 Quick videos covering basic ML algorithms Variational Autoencoders from a probabilistic perspective Convolutional Neural Networks Papers \u00b6 Subsurface Structure Analysis Using Computational Interpretation and Learning: A Visual Signal Processing Perspective. A comparison of classification techniques for seismic facies recognition","title":"List of Resources"},{"location":"resources/list/#list-of-resources","text":"TensorFlow","title":"List of Resources"},{"location":"resources/list/#tutorials","text":"Quick videos covering basic ML algorithms Variational Autoencoders from a probabilistic perspective Convolutional Neural Networks","title":"Tutorials"},{"location":"resources/list/#papers","text":"Subsurface Structure Analysis Using Computational Interpretation and Learning: A Visual Signal Processing Perspective. A comparison of classification techniques for seismic facies recognition","title":"Papers"}]}